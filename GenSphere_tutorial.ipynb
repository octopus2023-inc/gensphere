{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXFrttQQOgajNnWCYrUDhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/octopus2023-inc/gensphere/blob/main/GenSphere_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GenSphere tutorial**"
      ],
      "metadata": {
        "id": "TlQGkk0vfyoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This quick tutorial will walk you through the main functionalities of [GenSphere](https://github.com/octopus2023-inc/gensphere).\n",
        "\n",
        "We will follow a guided example, **where we create a workflow that finds what are the latest product releases at [producthunt.com](http//producthunt.com), searches for traction information like revenue, number of users, and analyzes a new startup idea based on that.**\n",
        "\n",
        "By completing this tutorial, you will learn about the main functionalities of GenSphere, such as:\n",
        "\n",
        "\n",
        "1.   Defining workflows with yaml files;\n",
        "2.   Pulling from the platform;\n",
        "3.   Nesting workflows;\n",
        "4.   Using custom functions and schemas, as well as using langchain and composio tools;\n",
        "5.   Visualizing workflows;\n",
        "6.   Pushing to the platform.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7lyQrTqqf4dR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **0. Install GenSphere and other libs to be used**"
      ],
      "metadata": {
        "id": "mvglv1mRhjhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jRwRLFfHn7br",
        "outputId": "9f73012f-703f-4981-de86-8950e4d9e43b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.4 starlette-0.41.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "330uMhRrfwVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c73cb3ab-3f73-4d75-a41d-08cb201b9894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://test.pypi.org/simple/\n",
            "Collecting gensphere==0.1.8\n",
            "  Downloading https://test-files.pythonhosted.org/packages/28/11/896662778f2fff0377c73f9955333ba13d374177048ee02f643cb02a84d4/gensphere-0.1.8-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: Jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from gensphere==0.1.8) (3.1.4)\n",
            "Collecting composio-core<0.6.0,>=0.5.35 (from gensphere==0.1.8)\n",
            "  Downloading composio_core-0.5.37-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting composio-openai<0.6.0,>=0.5.35 (from gensphere==0.1.8)\n",
            "  Downloading composio_openai-0.5.37-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting dash<3.0.0,>=2.18.1 (from gensphere==0.1.8)\n",
            "  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dash-cytoscape<2.0.0,>=1.0.2 (from gensphere==0.1.8)\n",
            "  Downloading dash_cytoscape-1.0.2.tar.gz (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from gensphere==0.1.8) (0.3.4)\n",
            "Collecting langchain-community<0.4.0,>=0.3.3 (from gensphere==0.1.8)\n",
            "  Downloading https://test-files.pythonhosted.org/packages/c8/4c/28602f96a6b5a2cad9ad474906a6eff0c65550bed844ce6c81478589a089/langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from gensphere==0.1.8) (3.4.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.52.2 in /usr/local/lib/python3.10/dist-packages (from gensphere==0.1.8) (1.52.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from gensphere==0.1.8) (2.9.2)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from gensphere==0.1.8)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2.32.3)\n",
            "Requirement already satisfied: jsonschema<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (4.23.0)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2.17.0)\n",
            "Collecting pysher==1.0.8 (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading Pysher-1.0.8.tar.gz (9.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (8.5.0)\n",
            "Collecting jsonref>=1.1.0 (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting inflection>=0.5.1 (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading inflection-0.5.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting semver>=2.13.0 (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading https://test-files.pythonhosted.org/packages/ee/d8/a89ff3ce50297ea8f9bbfb86cb86170a227aad8cf8bf48f4b739e56a4591/semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (8.1.7)\n",
            "Requirement already satisfied: rich<14,>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (13.9.3)\n",
            "Requirement already satisfied: pyperclip<2,>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (1.9.0)\n",
            "Collecting paramiko>=3.4.1 (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading paramiko-3.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (0.115.4)\n",
            "Collecting uvicorn (from composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: websocket-client!=0.49 in /usr/local/lib/python3.10/dist-packages (from pysher==1.0.8->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (1.8.0)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash<3.0.0,>=2.18.1->gensphere==0.1.8) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash<3.0.0,>=2.18.1->gensphere==0.1.8) (3.0.6)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash<3.0.0,>=2.18.1->gensphere==0.1.8) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.18.1->gensphere==0.1.8)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.18.1->gensphere==0.1.8)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.18.1->gensphere==0.1.8)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash<3.0.0,>=2.18.1->gensphere==0.1.8) (4.12.2)\n",
            "Collecting retrying (from dash<3.0.0,>=2.18.1->gensphere==0.1.8)\n",
            "  Downloading https://test-files.pythonhosted.org/packages/7f/d3/3bde2355eeefec4befe0ce400ae847619544b57172e00e95fba92cf4d0ac/retrying-1.3.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash<3.0.0,>=2.18.1->gensphere==0.1.8) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash<3.0.0,>=2.18.1->gensphere==0.1.8) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4.0.0,>=3.1.4->gensphere==0.1.8) (3.0.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (2.0.36)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (1.26.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.3->gensphere==0.1.8)\n",
            "  Downloading https://test-files.pythonhosted.org/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.3->gensphere==0.1.8)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.4 (from gensphere==0.1.8)\n",
            "  Downloading https://test-files.pythonhosted.org/packages/9a/d9/c66b02e278a20db6d931ee500538f82c18a8f30fae0a9dd7d965dd5a286d/langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain<0.4.0,>=0.3.4->gensphere==0.1.8)\n",
            "  Downloading https://test-files.pythonhosted.org/packages/cf/0e/9b0c2214a371e99d26586c7a61f8c8af5b1150d46c8fa10d9b58e3c5bfa2/langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.3->gensphere==0.1.8)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.2->gensphere==0.1.8) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.2->gensphere==0.1.8) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.2->gensphere==0.1.8) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.2->gensphere==0.1.8) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.2->gensphere==0.1.8) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.2->gensphere==0.1.8) (4.66.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.2->gensphere==0.1.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.2->gensphere==0.1.8) (2.23.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.2->gensphere==0.1.8) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.2->gensphere==0.1.8) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.3->gensphere==0.1.8)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.3->gensphere==0.1.8)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.18.1->gensphere==0.1.8) (2.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.2->gensphere==0.1.8) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.2->gensphere==0.1.8) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.2->gensphere==0.1.8) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.8.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (3.20.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4.21.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4.21.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4.21.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (0.20.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (1.0.0)\n",
            "Collecting bcrypt>=3.2 (from paramiko>=3.4.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=3.4.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (43.0.3)\n",
            "Collecting pynacl>=1.5 (from paramiko>=3.4.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2.2.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=13.7.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=13.7.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (3.1.1)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (0.41.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash<3.0.0,>=2.18.1->gensphere==0.1.8) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=3.4.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (1.17.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain<0.4.0,>=0.3.4->gensphere==0.1.8) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=13.7.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.3->gensphere==0.1.8)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (0.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=3.4.1->composio-core<0.6.0,>=0.5.35->gensphere==0.1.8) (2.22)\n",
            "Downloading https://test-files.pythonhosted.org/packages/28/11/896662778f2fff0377c73f9955333ba13d374177048ee02f643cb02a84d4/gensphere-0.1.8-py3-none-any.whl (26 kB)\n",
            "Downloading composio_core-0.5.37-py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.4/439.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading composio_openai-0.5.37-py3-none-any.whl (4.9 kB)\n",
            "Downloading dash-2.18.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading https://test-files.pythonhosted.org/packages/c8/4c/28602f96a6b5a2cad9ad474906a6eff0c65550bed844ce6c81478589a089/langchain_community-0.3.4-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://test-files.pythonhosted.org/packages/9a/d9/c66b02e278a20db6d931ee500538f82c18a8f30fae0a9dd7d965dd5a286d/langchain-0.3.6-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading https://test-files.pythonhosted.org/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading https://test-files.pythonhosted.org/packages/cf/0e/9b0c2214a371e99d26586c7a61f8c8af5b1150d46c8fa10d9b58e3c5bfa2/langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paramiko-3.5.0-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading https://test-files.pythonhosted.org/packages/ee/d8/a89ff3ce50297ea8f9bbfb86cb86170a227aad8cf8bf48f4b739e56a4591/semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading https://test-files.pythonhosted.org/packages/7f/d3/3bde2355eeefec4befe0ce400ae847619544b57172e00e95fba92cf4d0ac/retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pysher, dash-cytoscape\n",
            "  Building wheel for pysher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysher: filename=Pysher-1.0.8-py3-none-any.whl size=9889 sha256=8f756a445854d16a657f22dda10c0811d113ca14c15562a095523136875a241a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/90/02/9ec52ec3bf45fbabf42b726d403ffe8bce46dd945befd7fe32\n",
            "  Building wheel for dash-cytoscape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-cytoscape: filename=dash_cytoscape-1.0.2-py3-none-any.whl size=4010716 sha256=9f1510d5ed4743597c3f81d569a6d42df223d8344d7aacfef9ebfc60dec0d2be\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/23/5e/56fa701c668444b121ad2353a96478179dc49086a9c44ee930\n",
            "Successfully built pysher dash-cytoscape\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, uvicorn, semver, retrying, python-dotenv, mypy-extensions, marshmallow, jsonref, inflection, httpx-sse, bcrypt, typing-inspect, pysher, pynacl, pydantic-settings, paramiko, dataclasses-json, dash, langchain-core, dash-cytoscape, composio-core, composio-openai, langchain, langchain-community, gensphere\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "Successfully installed bcrypt-4.2.0 composio-core-0.5.37 composio-openai-0.5.37 dash-2.18.1 dash-core-components-2.0.0 dash-cytoscape-1.0.2 dash-html-components-2.0.0 dash-table-5.0.0 dataclasses-json-0.6.7 gensphere-0.1.8 httpx-sse-0.4.0 inflection-0.5.1 jsonref-1.1.0 langchain-0.3.6 langchain-community-0.3.4 langchain-core-0.3.15 marshmallow-3.23.1 mypy-extensions-1.0.0 paramiko-3.5.0 pydantic-settings-2.6.1 pynacl-1.5.0 pysher-1.0.8 python-dotenv-1.0.1 retrying-1.3.4 semver-3.0.2 typing-inspect-0.9.0 uvicorn-0.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --extra-index-url https://test.pypi.org/simple/ gensphere==0.1.8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Import GenSphere**"
      ],
      "metadata": {
        "id": "cLMJ6JWqp4Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import traceback\n",
        "\n",
        "\n",
        "# Set up logging configuration before importing other modules\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"../../app.log\", mode='w'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "fJ_x1QT_OUgD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensphere import genflow, yaml_utils\n",
        "from gensphere.genflow import GenFlow\n",
        "from gensphere.yaml_utils import YamlCompose\n",
        "from gensphere.visualizer import Visualizer\n",
        "from gensphere.hub import Hub\n",
        "import dotenv\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "B3MaB28Rp8IC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Define your enviroment variables**"
      ],
      "metadata": {
        "id": "aY3K4vfihwJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace these env variables with your corresponding API key."
      ],
      "metadata": {
        "id": "j6XRCre6h0-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=\"PLACE-YOUR-OPENAI-API-KEY\"\n",
        "os.environ['COMPOSIO_API_KEY']=\"PLACE-YOUR-COMPOSIO-API-KEY\" #if you don't have one, visit composio.dev\n",
        "os.environ['FIRECRAWL_API_KEY']=\"PLACE-YOUR-FIRECRAWL-API-KEY\" # if you don't have one, visit firecrawl.dev"
      ],
      "metadata": {
        "id": "JtxHn-sbhzqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96c1413-2591-4a3d-ed24-117c0c972b3f",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!composio add firecrawl #for this project, we will be using firecrawl. Get an API key and add it by following the steps here."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9nSmz830ysD",
        "outputId": "42e6a264-08a1-444a-a57a-3e47223abdc3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32m> Adding integration: Firecrawl\u001b[0m\u001b[32m...\u001b[0m\n",
            "\n",
            "> Enter API Key: fc-02f456f0cc524af393002a0514c4ed73\n",
            "> Enter Base URL (Optional):\n",
            "> Enter API Key: fc-02f456f0cc524af393002a0514c4ed73\n",
            "> Enter Base URL (Optional):\n",
            "✔ firecrawl added successfully with ID: 891f8bc7-64b5-4a84-91e9-d1398aa9b9cf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Define your workflow with a yaml file.**"
      ],
      "metadata": {
        "id": "1b1-N3bJip6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our aim is to create workflow that automatically finds **latest product releases from producthunt, explores their revenue and traction, and analyzes a new startup idea based on that**. We will use pre-built components from the platform to accelerate our development."
      ],
      "metadata": {
        "id": "vFPWTV3c6EtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GenSphere project structure**"
      ],
      "metadata": {
        "id": "wScKRAjqddKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 fundamental files in a GenSphere project.\n",
        "\n",
        "\n",
        "1.   **Yaml file** - contains the workflow definition\n",
        "2.   **Functions file** - .py file containing all functions to be used, either as nodes in the graph or as tools during LLM function calling\n",
        "3.   **Schemas file** - .py file containing pydantic schemas. These are used when you want to use structured outputs from openAI.\n",
        "\n"
      ],
      "metadata": {
        "id": "09Z3TLZZdllN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1 Pull a base yaml file from the platform**"
      ],
      "metadata": {
        "id": "AR10QjIzkPl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will use a **pre-built workflow from our open platform** that extracts information from producthunt.com. We will nest that into a bigger workflow to achieve our objective of analyzing a new startup idea.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHLs_l1sofcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_save_yaml_file='product_hunt_analyzer.yaml'\n",
        "path_to_save_functions_file='gensphere_functions.py'\n",
        "path_to_save_schema_file='structured_output_schema.py'\n",
        "\n",
        "hub=Hub()\n",
        "hub.pull(push_id='de8afbeb-06cb-4f8f-8ead-64d9e6ef5326',\n",
        "         yaml_filename=path_to_save_yaml_file,\n",
        "         functions_filename=path_to_save_functions_file,\n",
        "         schema_filename=path_to_save_schema_file,\n",
        "         save_to_disk=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pvZ4XdDP8Qif",
        "outputId": "f9401599-34d5-4269-b982-d3168bd0f1dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'yaml_file.yaml': '# product_hunt_analyzer.yaml\\r\\n\\r\\nnodes:      \\r\\n  - name: get_current_date\\r\\n    type: function_call\\r\\n    function: get_current_date_function\\r\\n    outputs:\\r\\n      - current_date\\r\\n      \\r\\n  - name: get_timewindow\\r\\n    type: function_call\\r\\n    function: get_timewindow_function\\r\\n    outputs:\\r\\n      - time_window\\r\\n  \\r\\n  - name: product_hunt_scrape\\r\\n    type: llm_service\\r\\n    service: openai\\r\\n    model: \"gpt-4o-2024-08-06\"\\r\\n    tools:\\r\\n      - COMPOSIO.FIRECRAWL_SCRAPE\\r\\n    params:\\r\\n      prompt: |\\r\\n         You should visit producthunt at https://www.producthunt.com/leaderboard/monthly/yyyy/mm \\r\\n         Today is {{ get_current_date.current_date }}\\r\\n         You should subsitute yyyy and mm by year and month you want to search.\\r\\n         The search time window should be {{ get_timewindow.time_window }}.\\r\\n         After that, you should extract raw content from the htmls associated,\\r\\n         which will contain information about new product launches, their companies, number of upvotes, etc. \\r\\n         Scroll the page until the end and wait a few miliseconds for it to launch before scraping.\\r\\n    outputs:\\r\\n      - product_hunt_scrape_results    \\r\\n      \\r\\n  - name: extract_info_from_search\\r\\n    type: llm_service\\r\\n    service: openai\\r\\n    model: \"gpt-4o-2024-08-06\"\\r\\n    structured_output_schema: StartupInformationList\\r\\n    params:\\r\\n      prompt: |\\r\\n         You are given reports from a search to https://www.producthunt.com/leaderboard/monthly/, containing \\r\\n         products featured there last month:\\r\\n         {{ product_hunt_scrape.product_hunt_scrape_results }}. \\r\\n         We want to extract accurate information about these new product launches. \\r\\n         Structure the information there by the following dimensions:  product name, company name, company url, number of upvotes, business model\\r\\n         brief description of it.\\r\\n    outputs:\\r\\n      - structured_search_info\\r\\n      \\r\\n  - name: postprocess_search_results\\r\\n    type: function_call\\r\\n    function: postprocess_search_results_functions\\r\\n    params:\\r\\n      info: \\'{{ extract_info_from_search.structured_search_info }}\\'\\r\\n    outputs:\\r\\n      - postprocessed_search_results\\r\\n      \\r\\n  - name: find_extra_info\\r\\n    type: llm_service\\r\\n    service: openai\\r\\n    model: \"gpt-4o-2024-08-06\"\\r\\n    tools:\\r\\n      - COMPOSIO.TAVILY_TAVILY_SEARCH\\r\\n    params:\\r\\n      prompt: |\\r\\n         You should conduct a comprehensive search on the web about the following entry from producthunt.com:\\r\\n         {{ postprocess_search_results.postprocessed_search_results[i] }}. You should look to find relevant news \\r\\n         about the company, specially related to its revenue, valuation, traction, acquisition if applicable, \\r\\n         number of users, etc.\\r\\n    outputs:\\r\\n      - startup_extra_info',\n",
              " 'structured_output_schema.py': '# advanced_search_schemas.py\\r\\n\\r\\nfrom pydantic import BaseModel, Field\\r\\nfrom typing import List\\r\\n\\r\\nclass StartupInformation(BaseModel):\\r\\n    product_name: str = Field(..., description=\"The name of the product\")\\r\\n    company_name: str = Field(..., description=\"The name of the company that offers the product. Could be equal to name of the product\")\\r\\n    url: str = Field(..., description=\"URL associated with the product.\")\\r\\n    number_upvotes: int = Field(..., description=\"Number of upvotes associated with the product\")\\r\\n    business_model: str = Field(..., description=\"A brief description about the business model of the product or company\")\\r\\n    brief_description: str = Field(..., description=\"A brief description about the product\")\\r\\n\\r\\nclass StartupInformationList(BaseModel):\\r\\n    information_list:List[StartupInformation]',\n",
              " 'gensphere_functions.py': '# product_hunt_search_functions.py\\r\\nfrom datetime import datetime\\r\\n\\r\\ndef read_file_as_string(file_path):\\r\\n    try:\\r\\n        with open(file_path, \\'r\\') as file:\\r\\n            content = file.read()\\r\\n        return {\\'domains\\':content}\\r\\n    except FileNotFoundError:\\r\\n        return \"Oops! It seems the specified file doesn\\'t exist. Please provide a valid file path.\"\\r\\n        \\r\\ndef get_current_date_function():\\r\\n    return {\\'current_date\\':datetime.today().strftime(\\'%Y-%m-%d\\')}\\r\\n    \\r\\ndef get_timewindow_function():\\r\\n    return {\\'time_window\\':\\'past month\\'}\\r\\n        \\r\\ndef postprocess_search_results_functions(info):\\r\\n    result=info.model_dump().get(\\'information_list\\')\\r\\n    return {\\'postprocessed_search_results\\':result}'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The yaml file has been saved locally as **\"product_hunt_analyzer.yaml\"**. We also saved the functions and schema files as **gensphere_functions.py** and **structured_output_schema.py**. Here are the full contents of the yaml file:"
      ],
      "metadata": {
        "id": "XP4ZGMVkEGSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# product_hunt_analyzer.yaml\n",
        "\n",
        "nodes:      \n",
        "\n",
        "  - name: get_current_date\n",
        "    type: function_call\n",
        "    function: get_current_date_function\n",
        "    outputs:\n",
        "      - current_date\n",
        "\n",
        "  - name: get_timewindow\n",
        "\n",
        "    type: function_call\n",
        "    function: get_timewindow_function\n",
        "    outputs:\n",
        "      - time_window\n",
        "\n",
        "  - name: product_hunt_scrape\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    tools:\n",
        "      - COMPOSIO.FIRECRAWL_SCRAPE\n",
        "    params:\n",
        "      prompt: |\n",
        "\n",
        "         You should visit producthunt at https://www.producthunt.com/leaderboard/monthly/yyyy/mm\n",
        "         Today is {{ get_current_date.current_date }}\n",
        "         You should subsitute yyyy and mm by year and month you want to search.\n",
        "         The search time window should be {{ get_timewindow.time_window }}.\n",
        "         After that, you should extract raw content from the htmls associated,\n",
        "         which will contain information about new product launches, their companies, number of upvotes, etc.\n",
        "         Scroll the page until the end and wait a few miliseconds for it to launch before scraping.\n",
        "\n",
        "    outputs:\n",
        "      - product_hunt_scrape_results    \n",
        "\n",
        "      \n",
        "  - name: extract_info_from_search\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    structured_output_schema: StartupInformationList\n",
        "    params:\n",
        "      prompt: |\n",
        "\n",
        "         You are given reports from a search to https://www.producthunt.com/leaderboard/monthly/, containing\n",
        "         products featured there last month:\n",
        "         {{ product_hunt_scrape.product_hunt_scrape_results }}.\n",
        "         We want to extract accurate information about these new product launches.\n",
        "         Structure the information there by the following dimensions:  product name, company name, company url, number of upvotes, business model\n",
        "         brief description of it.\n",
        "    outputs:\n",
        "\n",
        "      - structured_search_info\n",
        "\n",
        "  - name: postprocess_search_results\n",
        "    type: function_call\n",
        "    function: postprocess_search_results_functions\n",
        "    params:\n",
        "      info: '{{ extract_info_from_search.structured_search_info }}'\n",
        "    outputs:\n",
        "      - postprocessed_search_results\n",
        "\n",
        "      \n",
        "\n",
        "  - name: find_extra_info\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    tools:\n",
        "\n",
        "      - COMPOSIO.TAVILY_TAVILY_SEARCH\n",
        "\n",
        "    params:\n",
        "      prompt: |\n",
        "\n",
        "         You should conduct a comprehensive search on the web about the following entry from producthunt.com:\n",
        "         {{ postprocess_search_results.postprocessed_search_results[i] }}. You should look to find relevant news\n",
        "         about the company, specially related to its revenue, valuation, traction, acquisition if applicable, number of users, etc.\n",
        "\n",
        "    outputs:\n",
        "      - startup_extra_info\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "c7F7ZjQMETrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2 Visualize your project**"
      ],
      "metadata": {
        "id": "ZxGi5xVUdMjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze the project we just pulled by using the visualizer class. **You can zoom in and out, and by clicking on a node, you can see all functions and schemas, inputs and outputs associated with it.**\n",
        "\n",
        "**OBS**: It is slightly cumbersome to visualize the graph from inside google colab. If you run locally, you can determine the address where the visualization will be run and access it through your browser."
      ],
      "metadata": {
        "id": "W5xEoi-FehOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viz=Visualizer('product_hunt_analyzer.yaml',\n",
        "               'gensphere_functions.py',\n",
        "               'structured_output_schema.py',\n",
        "               address='127.0.0.1', port=8050)\n",
        "viz.start_visualization()"
      ],
      "metadata": {
        "id": "GOHf80RUexbg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "outputId": "49367945-1e4e-42f1-ab8f-652559636687"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "INFO:gensphere.graph_builder:Total elements generated: 11\n",
            "/usr/local/lib/python3.10/dist-packages/dash/dash.py:2282: DeprecationWarning:\n",
            "\n",
            "Dash.run_server is deprecated and will be removed in Dash 3.0\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.3 Understand the syntax of the yaml file**"
      ],
      "metadata": {
        "id": "hBbISMzNpevC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's understand how to work with YAML files step by step. **There are 3 types of node types: function_call, llm_service and yml_flow.**"
      ],
      "metadata": {
        "id": "j6XA2_NTaW_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.3.1 **function_call nodes**\n",
        "\n",
        "function_call nodes trigger function execution defined on a .py file (which you will pass when triggering execution). They have a **params field and output fields**.\n",
        "\n",
        "For instance, have a look at the get_current_date node:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  - name: get_current_date\n",
        "    type: function_call\n",
        "    function: get_current_date_function\n",
        "    outputs:\n",
        "      - current_date\n",
        "```\n",
        "Here we are instructing GenSphere to execute the function get_current_date_function, and in this case there are no 'params'. This functions is defined on gensphere_functions.py which was pulled from the platform together with the yaml file.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# gensphere_functions.py\n",
        "\n",
        "import datetime\n",
        "\n",
        "def get_current_date_function():\n",
        "    return {'current_date':datetime.today().strftime('%Y-%m-%d')}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Important notes**:  \n",
        "\n",
        "1.   If you want to use other nodes outputs as inputs, you can reference them with the syntax **{{ node name.output_name }}** in the 'params' field of the node.\n",
        "2.   **Functions output must be a dict**, whose keys must match the outputs defined in the yaml file.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VpYH2gE4S0x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.3.2 llm_service nodes**\n",
        "\n",
        "These nodes execute LLM API calls. In the current version, we only support openAI, including [structured outputs](https://https://openai.com/index/introducing-structured-outputs-in-the-api/) and [function calling](https://https://platform.openai.com/docs/guides/function-calling). For instance, have a look at the node product_hunt_scrape:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#yaml_file.yaml\n",
        "\n",
        "   - name: product_hunt_scrape\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    tools:\n",
        "      - COMPOSIO.FIRECRAWL_SCRAPE\n",
        "    params:\n",
        "      prompt: |\n",
        "         You should visit producthunt at https://www.producthunt.com/leaderboard/monthly/yyyy/mm\n",
        "         Today is {{ get_current_date.current_date }}\n",
        "         You should subsitute yyyy and mm by year and month you want to search.\n",
        "         The search time window should be {{ get_timewindow.time_window }}.\n",
        "         After that, you should extract raw content from the htmls associated,\n",
        "         which will contain information about new product launches, their companies, number of upvotes, etc.\n",
        "         Scroll the page until the end and wait a few miliseconds for it to launch before scraping.\n",
        "    outputs:\n",
        "      - product_hunt_scrape_results\n",
        "```\n",
        "The **tools** field can refer to any function on your .py that defines functions.\n",
        "\n",
        "**You can also use [Composio](https://composio.dev/) tools**, with the syntax \"COMPOSIO.composio_tool_name\". Check [Composio's documentation](https://app.composio.dev/sdk_guide) for a detailed view on all available tools.\n",
        "\n",
        "**We also support [Langchain](https://langchain.com) tools**. You can use any tool from [langchain_community.tools](https://python.langchain.com/api_reference/community/tools.html) with the syntax \"LANGCHAIN.langchain_tool_name\"\n",
        "\n",
        "If you want your output from openAI to be a dict with a predetermined schema, **you can use the structured_output_schema field**, as in the node 'extract_info_from_search':\n",
        "\n",
        "```\n",
        "#product_hunt_analyzer.yaml\n",
        "\n",
        "  - name: extract_info_from_search\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    structured_output_schema: StartupInformationList\n",
        "    params:\n",
        "      prompt: |\n",
        "         You are given reports from a search to https://www.producthunt.com/leaderboard/monthly/, containing\n",
        "         products featured there in the following time window: {{ get_timewindow.time_window }}. Here\n",
        "         is the content of the search:\n",
        "         {{ product_hunt_scrape.product_hunt_scrape_results }}.\n",
        "         We want to extract accurate information about these new product launches.\n",
        "         Structure the information there by the following dimensions:  product name, company name, company url, number of upvotes, business model\n",
        "         brief description of it.\n",
        "    outputs:\n",
        "      - structured_search_info\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XH2Rlaj_UAKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will be an instance of the class **StartupInformationList**, which is defined on **structured_output_schemas.py** as\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# structured_output_schema.py\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class StartupInformation(BaseModel):\n",
        "    product_name: str = Field(..., description=\"The name of the product\")\n",
        "    company_name: str = Field(..., description=\"The name of the company that offers the product. Could be equal to name of the product\")\n",
        "    url: str = Field(..., description=\"URL associated with the product.\")\n",
        "    number_upvotes: int = Field(..., description=\"Number of upvotes associated with the product\")\n",
        "    business_model: str = Field(..., description=\"A brief description about the business model of the product or company\")\n",
        "    brief_description: str = Field(..., description=\"A brief description about the product\")\n",
        "\n",
        "class StartupInformationList(BaseModel):\n",
        "    information_list:List[StartupInformation]\n",
        "```\n",
        "\n",
        "The output of nodes with structured_output_schema are instances of the class defined on the schemas file (structured_output_schema.py in our case). To reference this output on other nodes, it is useful to introduce an additional post-processing node to extract information we want from the class instance. That's the purpose of the postprocess_search_results node:\n",
        "\n",
        "```\n",
        "  - name: postprocess_search_results\n",
        "    type: function_call\n",
        "    function: postprocess_search_results_functions\n",
        "    params:\n",
        "      info: '{{ extract_info_from_search.structured_search_info }}'\n",
        "    outputs:\n",
        "      - postprocessed_search_results\n",
        "```\n",
        "\n",
        "which applies the function **postprocess_search_results_functions**, defined on **gensphere_functions.py**.\n",
        "\n",
        "```\n",
        "def postprocess_search_results_functions(info):\n",
        "    result=info.model_dump().get('information_list')\n",
        "    return {'postprocessed_search_results':result}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bwD7tCloqFll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.3.3 yml_flow nodes**\n",
        "\n",
        "These nodes represent entire yaml files themselves. So, you can easily nest workflows by referencing other yaml files here. We will see an example of yaml file that contains yaml nodes below. For now, have a look at an example of a yml_flow node\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "- name: example_node_name\n",
        "    type: yml_flow\n",
        "    yml_file: path-to-yaml-file\n",
        "    params:\n",
        "      yml_flow_argument_example: 'xyz'\n",
        "    outputs:\n",
        "      - yml_flow_output_example\n",
        "```\n",
        "\n",
        "when referecing yml_flow nodes inside your yaml file, GenSphere will handle dependencies and **compose a combined yaml file** that is ready to run.\n",
        "\n"
      ],
      "metadata": {
        "id": "utah1ws-cZpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3.3.4 Working with lists**\n",
        "\n",
        "Many times, the output of a node will be a python list and we will want to apply the next node to each individual element of the list.\n",
        "\n",
        "You can easily accomplish this with the syntax by **appending [i] after a node reference, as in {{node_name.output_name[i] }}**. If a node references that (either in its 'params' field or in the 'prompt' field for llm_service nodes), **GenSphere will execute the node to each element of the iterable and collect output as a list**. For instance, lets examine the node 'find_extra_info':\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  - name: find_extra_info\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    tools:\n",
        "      - COMPOSIO.TAVILY_TAVILY_SEARCH\n",
        "    params:\n",
        "      prompt: |\n",
        "         You should conduct a comprehensive search on the web about the following entry from producthunt.com:\n",
        "         {{ postprocess_search_results.postprocessed_search_results[i] }}. You should look to find relevant news\n",
        "         about the company, specially related to its revenue, valuation, traction, acquisition if applicable,\n",
        "         number of users, etc.\n",
        "    outputs:\n",
        "      - startup_extra_info\n",
        "```\n",
        "\n",
        "Notice that the 'prompt' field references the output of the node 'postprocess_search_results' as:\n",
        "\n",
        "```\n",
        "{{ postprocess_search_results.postprocessed_search_results[i] }}\n",
        "```\n",
        "\n",
        "That means this GenSphere will take each element of  \"postprocessed_search_results\" (which is a list, as defined by its structured_output_schema applied in the node extract_info_from_search) and apply \"find_extra_info\" to every element of this node. **The outputs are then collected as a list**\n",
        "\n",
        "The end result is that we will do a different LLM API call for each entry that we found on product hunt separately. By doing so, we will get much better results than if we tried to find information about all entries at once.\n"
      ],
      "metadata": {
        "id": "YC3WuVythx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.4 Combine workflows to compose final yaml**"
      ],
      "metadata": {
        "id": "i6Er-bYL-_70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The workflow we saw so far is able to retrieve information from product hunt and perform some extra web research on  companies there to find their revenue, number of users etc. **Now, we will embed this into a larger workflow** that takes a startup idea as input, runs the product hunt search workflow and creates a report explaining if there are potential competitors to the idea on recent product hunt launches, what are some market trends etc.\n",
        "\n",
        "Let's start with a new yaml file. It is already saved locally in the repo, inside the examples folder, at **https://raw.githubusercontent.com/octopus2023-inc/gensphere/refs/heads/main/examples/startup_idea_evaluator.yaml**"
      ],
      "metadata": {
        "id": "uiVGIkGKP6U5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "#startup_idea_evaluator.yaml\n",
        "\n",
        "nodes:\n",
        "  - name: read_idea\n",
        "    type: function_call\n",
        "    function: read_file_as_string\n",
        "    params:\n",
        "      file_path: \"domains_to_search.txt\"\n",
        "    outputs:\n",
        "      - domains\n",
        "      \n",
        "  - name: product_hunt_analyzer\n",
        "    type: yml_flow\n",
        "    yml_file: product_hunt_analyzer.yaml\n",
        "    outputs:\n",
        "      - postprocessed_search_results\n",
        "      - startup_extra_info\n",
        "      \n",
        "  - name: generate_report\n",
        "    type: llm_service\n",
        "    service: openai\n",
        "    model: \"gpt-4o-2024-08-06\"\n",
        "    params:\n",
        "      prompt: |\n",
        "         You are a world class VC analyst. You are currently analyzing the following startup idea:\n",
        "         {{ read_idea.domains }}\n",
        "         Your task is to help analyze this idea in face of recent launches in product hunt.\n",
        "         Some recents launches in producthunt.com are:\n",
        "         {{ product_hunt_analyzer.postprocessed_search_results }}\n",
        "         Besides that, some extra information about these companies is:\n",
        "         {{ product_hunt_analyzer.startup_extra_info }}.\n",
        "        \n",
        "         Given that, you should create a detailed report containing the following:\n",
        "         1. An overview of recent launches in producthunt.com. What are the main ideas being explored?\n",
        "         2. A list of companies from producthunt launches that may become direct competitors to the startup idea.\n",
        "         Explain your rational\n",
        "         3. Create a list of the most promising startups from the producthunt launches, as defined by their\n",
        "         valuation, revenue, traction or other relevant metrics.\n",
        "         4. A table containing all information you found from producthunt launches.\n",
        "         \n",
        "         Answer in markdown format.\n",
        "         \n",
        "    outputs:\n",
        "      - report\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tCeksmClSYYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we have a **yml_flow** node being referenced here,\n",
        "\n",
        "```\n",
        "  - name: product_hunt_analyzer\n",
        "    type: yml_flow\n",
        "    yml_file: yaml_file.yaml\n",
        "    outputs:\n",
        "      - postprocessed_search_results\n",
        "      - startup_extra_info\n",
        "```\n",
        "\n",
        "In the yml_file field, we have yaml_file.yaml which is the path to the yaml file with the product analysis we were looking before. **That means this node will trigger the execution of the entire workflow defined on yaml_file.yaml.**\n",
        "\n",
        "Now let's a new yaml, which we name \"combined.yaml\" with YamlCompose. GenSphere's class YamlCompose receives as input a yaml file, looks for yml_flow nodes there and resolves dependencies to create a final yaml file that is ready to run."
      ],
      "metadata": {
        "id": "Ksoro8mrVG4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy the file from repo to local working directory in the notebook\n",
        "!wget -O startup_idea_evaluator.yaml https://raw.githubusercontent.com/octopus2023-inc/gensphere/refs/heads/main/examples/startup_idea_evaluator.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XFhbtx6P86WY",
        "outputId": "d38cd094-ca4d-409b-d586-149f67a74c53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-01 18:19:17--  https://raw.githubusercontent.com/octopus2023-inc/gensphere/refs/heads/main/examples/startup_idea_evaluator.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1780 (1.7K) [text/plain]\n",
            "Saving to: ‘startup_idea_evaluator.yaml’\n",
            "\n",
            "\r          startup_i   0%[                    ]       0  --.-KB/s               \rstartup_idea_evalua 100%[===================>]   1.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-01 18:19:17 (20.7 MB/s) - ‘startup_idea_evaluator.yaml’ saved [1780/1780]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "composer=YamlCompose('startup_idea_evaluator.yaml',\n",
        "                     'gensphere_functions.py',\n",
        "                     'structured_output_schema.py')\n",
        "combined_yaml_data=composer.compose(save_combined_yaml=True, output_file='combined.yaml')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93YFjt1PUDTU",
        "outputId": "140f9f79-4d1b-487d-8929-3307a190f477"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n",
            "INFO:composio:Logging is set to INFO, use `logging_level` argument or `COMPOSIO_LOGGING_LEVEL` change this\n",
            "INFO:composio:Logging is set to INFO, use `logging_level` argument or `COMPOSIO_LOGGING_LEVEL` change this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** When calling YamlCompose, you need to pass also the functions and schema files of the yaml file you want to parse. For simplicity, in our case you defined all functions we would need on the functions and schema files we pulled from the platform gensphere_functions.py, structured_output_schema.py"
      ],
      "metadata": {
        "id": "Xc3wImC_Ui2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now visualize the combined yaml file, and check that the workflows have been correctly nested."
      ],
      "metadata": {
        "id": "bq0Xq-JoYLBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "viz=Visualizer('combined.yaml',\n",
        "               'gensphere_functions.py',\n",
        "               'structured_output_schema.py',\n",
        "               address='127.0.0.1', port=8050)\n",
        "viz.start_visualization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "VJmOoQAbYKcc",
        "outputId": "5448a6c3-8674-4619-c8d5-7e87682077bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n",
            "INFO:gensphere.graph_builder:Total elements generated: 16\n",
            "/usr/local/lib/python3.10/dist-packages/dash/dash.py:2282: DeprecationWarning:\n",
            "\n",
            "Dash.run_server is deprecated and will be removed in Dash 3.0\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Run your project**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CtgGsPgTjC-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having defined the yaml, functions and schema file, we can now trigger execution using the **GenFlow class**. We simply pass file paths to it and call the **\".run()\"** method.\n",
        "\n"
      ],
      "metadata": {
        "id": "7ngNboN8jJ8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first node of combined.yaml, read_idea, expects a txt file saved locally as **\"domains_to_search.txt\"**. Let's create this file before executing the flow:"
      ],
      "metadata": {
        "id": "k1j4dRClaM1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a save domains_to_search.txt\n",
        "\n",
        "startup_idea=\"\"\"\n",
        "startup that creates interactive voice agents using generative AI with emphasis on applications like\n",
        "language tutoring, entertainment or mental health. The business model would be B2C.\n",
        "\"\"\"\n",
        "with open(\"domains_to_search.txt\", \"w\") as text_file:\n",
        "    text_file.write(startup_idea)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gJsMSWKaeCi",
        "outputId": "05bdaf33-b228-4bcd-de55-7e92c21b82f3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger('composio').setLevel(logging.WARNING)\n",
        "logging.getLogger('gensphere').setLevel(logging.DEBUG)\n",
        "logging.getLogger('GenFlow').setLevel(logging.DEBUG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfpB-8fGSJgk",
        "outputId": "523e75bc-3dfc-45a6-cdba-a9b925df435e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flow=GenFlow('combined.yaml',\n",
        "             'gensphere_functions.py',\n",
        "             'structured_output_schema.py')\n",
        "flow.parse_yaml()\n",
        "flow.run()"
      ],
      "metadata": {
        "id": "EL228aVFjANp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f145c8b2-4900-4bba-bf65-4c8a6f54d863"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n",
            "DEBUG:gensphere.yaml_utils:Validating YAML file '/content/combined.yaml'\n",
            "INFO:gensphere.genflow:yaml file /content/combined.yaml passed all consistency checks\n",
            "INFO:gensphere.genflow:Execution order: ['read_idea', 'product_hunt_analyzer__get_current_date', 'product_hunt_analyzer__get_timewindow', 'product_hunt_analyzer__product_hunt_scrape', 'product_hunt_analyzer__extract_info_from_search', 'product_hunt_analyzer__postprocess_search_results', 'product_hunt_analyzer__find_extra_info', 'generate_report']\n",
            "INFO:gensphere.genflow.read_idea:Executing node 'read_idea'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__get_current_date:Executing node 'product_hunt_analyzer__get_current_date'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__get_timewindow:Executing node 'product_hunt_analyzer__get_timewindow'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__product_hunt_scrape:Executing node 'product_hunt_analyzer__product_hunt_scrape'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__extract_info_from_search:Executing node 'product_hunt_analyzer__extract_info_from_search'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__postprocess_search_results:Executing node 'product_hunt_analyzer__postprocess_search_results'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.product_hunt_analyzer__find_extra_info:Executing node 'product_hunt_analyzer__find_extra_info'\n",
            "INFO:gensphere.genflow.generate_report:Executing node 'generate_report'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After execution is complete, you can access results with the **.outputs** atribute of GenFlow, which returns a dict with every node as key, and their outputs as values."
      ],
      "metadata": {
        "id": "pkze3I8JlrtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_node_output=flow.outputs.get(\"generate_report\").get(\"report\")\n",
        "\n",
        "#visualize output\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(final_node_output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "QGgblEp2YsfG",
        "outputId": "36bfba82-e8c4-47d5-8e0f-146fd85ae185"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```markdown\n# VC Analyst Report: Analysis of Startup Idea and Comparison with Product Hunt Launches\n\n## 1. Overview of Recent Launches on Product Hunt\n\nRecent launches on Product Hunt primarily showcase products aimed at enhancing productivity, communication, and leveraging AI for various applications. Here are the main ideas being explored:\n\n- **AI-Powered Tools**: Many products leverage AI to improve efficiency and quality, such as Trag's AI-powered code review tool.\n- **Productivity and Collaboration Platforms**: Several startups focus on productivity, such as KYZON Space for meetings and General Collaboration for unified communications.\n- **Real-Time AI Interactions**: Video SDK 3.0 provides tools for creating real-time interactive video experiences.\n- **Data Analytics and Insights**: buzzabout provides marketing insights based on large-scale data analysis.\n\nThe trend suggests a significant interest in using AI to drive efficiency and provide enhanced user experiences across various sectors.\n\n## 2. Potential Direct Competitors\n\nBelow is a list of companies from the Product Hunt launches that could become direct competitors to the startup idea:\n\n- **Video SDK 3.0**: Although primarily focused on video environments, its features for real-time, interactive AI characters could overlap with creating interactive voice agents in entertainment and tutoring applications. The emphasis on immersive experiences through real-time AI can be considered a competitive aspect.\n  \n**Rationale**: Video SDK 3.0’s expertise in real-time AI integration offers potential competition, especially if they expand offerings into interactive voice experiences.\n\n## 3. Most Promising Startups\n\nThe most promising startups from the Product Hunt launches, based on available traction metrics and popularity, include:\n\n- **KYZON Space**: Although detailed financial data is lacking, its significant upvotes on Product Hunt suggest strong user interest and potential traction.\n- **Trag**: With high upvotes and robust functionality in AI-driven code review, this startup shows potential in the booming AI software tools sector.\n\n**Note**: Due to the lack of specific revenue or valuation metrics provided, the assessment is based primarily on user interest as indicated by Product Hunt upvotes.\n\n## 4. Table of Product Hunt Launches\n\nBelow is a summary table of key information concerning the recent Product Hunt launches:\n\n| Product Name      | Company Name           | URL                                | Number of Upvotes | Business Model                                           | Brief Description                                                                                                                                              |\n|-------------------|------------------------|------------------------------------|-------------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| KYZON Space       | KYZON                  | [kyzonspace.com](https://www.kyzonspace.com) | 1633              | Subscription-based SaaS for teams                         | Provides tools for transforming ideas into actionable meeting outcomes.                                                                                       |\n| Trag              | Trag                   | [trag.com](https://www.trag.com)   | 1614              | Freemium model with GitHub integration                    | AI-powered tool for code review enhancing code quality and efficiency.                                                                                       |\n| Video SDK 3.0     | Video SDK              | [videosdk.com](https://www.videosdk.com) | 1608              | Pay-as-you-go for developers                              | Allows integration of real-time AI characters for dynamic and interactive video environments.                                                                 |\n| buzzabout         | buzzabout              | [buzzabout.ai](https://www.buzzabout.ai) | 1319              | Tiered pricing for data analysis services                 | Provides insights from online discussions to empower strategic marketing decisions.                                                                           |\n| Feta              | Feta                   | [fetastandups.com](https://www.fetastandups.com) | 1195              | Subscription plans for team collaboration                 | Enhances stand-up meetings and productivity through advanced communication tools.                                                                             |\n| General Collaboration | General Collaboration | [generalcollaboration.com](https://www.generalcollaboration.com) | 1163              | Basic and premium collaboration plans                     | Centralizes work discussions to enhance productivity and coordination in remote work setups.                                                                  |\n| HeyForm 3.0       | HeyForm                | [heyform.com](https://www.heyform.com) | 1125              | Open-source with premium features                         | Open-source form builder tailored for small businesses.                                                                                                        |\n\n*Note: The analysis is constrained by the lack of detailed financial metrics and relies heavily on the popularity and user engagement metrics provided by Product Hunt upvotes.*\n```\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Push to the platform**"
      ],
      "metadata": {
        "id": "d6RQ30OH5Xzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you have finished your project, you can now push your yaml, functions and schema to the platform. This will generate a push_id, that you or anyone else can use to pull your project locally. To do that, we simply call hub.push(), passing the path to the files. You can also add a brief description with \"push_name\"."
      ],
      "metadata": {
        "id": "EtFeWfPk5aaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hub=Hub(yaml_file='combined.yaml',\n",
        "        functions_file='gensphere_functions.py',\n",
        "        schema_file='structured_output_schema.py')\n",
        "result=hub.push(push_name='workflow to analyze startup idea based on recent producthunt launches.')"
      ],
      "metadata": {
        "id": "8wJ_wewkkJZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2059829e-0b00-4cae-d992-f1a1e88d3477"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n",
            "DEBUG:gensphere.yaml_utils:Validating YAML file '/content/combined.yaml'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"push id is {result.get('push_id')}\")\n",
        "print(f\"uploaded files are {result.get('uploaded_files')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQV13P-9-3NU",
        "outputId": "53b76a5b-bb6a-48c1-cbe0-f1cdeab669b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "push id is dc6ba3ae-8221-4264-94e9-bb805d9a1365\n",
            "uploaded files are ['yaml_file.yaml', 'gensphere_functions.py', 'structured_output_schema.py']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Check project popularity**"
      ],
      "metadata": {
        "id": "n9fnIPd46VEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check how many times your project was pulled from the platform by using the 'count_pulls' method, and passing your push_id."
      ],
      "metadata": {
        "id": "KRmNLDZA6iz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of pulls for the push_id\n",
        "total_pulls = hub.count_pulls(push_id='de8afbeb-06cb-4f8f-8ead-64d9e6ef5326')\n",
        "print(f\"Total pulls for push_id: {total_pulls}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIcEFbMx6feu",
        "outputId": "6d6e1aea-8f42-4936-d5d4-c55f395cf740"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning:\n",
            "\n",
            "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pulls for push_id: 5\n"
          ]
        }
      ]
    }
  ]
}