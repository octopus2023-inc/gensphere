{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf10 GenSphere Documentation","text":"<p>Welcome to the official documentation for GenSphere, a declarative framework to build LLM applications and an open platform to push and pull them. Think of GenSphere as Docker for LLM applications.</p>"},{"location":"#introduction","title":"Introduction","text":""},{"location":"#what-is-gensphere","title":"What is GenSphere?","text":"<p>GenSphere is a framework that allows you to build Large Language Model (LLM) applications by declaring tasks and how they connect using YAML files. It breaks down any LLM application into graphs where each node is either a function call, an LLM API call, or another graph itself. This approach provides:</p> <ul> <li>Low-Level Control: Inspect and edit applications down to their core components, avoiding unnecessary abstractions.</li> <li>Portability: Projects are defined by YAML files and associated Python functions and schemas, making sharing easy.</li> <li>Community Collaboration: Push your application to an open platform (no registration required) and generate a public ID to make it accessible to anyone.</li> <li>Composability: Easily compose complex applications from simpler, reusable components.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Define Workflows with Simple YAML Files: Create complex execution graphs using simple YAML files.</li> <li>Gain Low-Level Control: Break workflows down to individual function calls and/or AI API calls.</li> <li>Nest LLM Applications Easily: Reference other YAML files as nodes in your workflow.</li> <li>Push and Pull Projects to the Open Community Hub: Collaborate by publishing and pulling projects from the platform.</li> <li>Track Popularity of Your Projects: Check how popular your projects are by the number of times they are used by others.</li> <li>Visualize Workflows: Explore your projects with interactive graphical visualization.</li> </ul>"},{"location":"#why-use-gensphere","title":"Why Use GenSphere?","text":"<p>GenSphere provides a transparent and flexible way to build LLM applications without the cumbersome abstractions that some modern frameworks introduce. It promotes collaboration and reuse by enabling developers to share and compose workflows easily.</p>"},{"location":"#how-does-gensphere-work","title":"How Does GenSphere Work?","text":"<ol> <li>Define Your Workflow with YAML Files: Your project is defined by YAML files representing graphs where each node is a task to be executed.</li> <li>Compose Complex Workflows by Nesting Graphs: Reference other YAML files as nodes to build upon existing workflows.</li> <li>Define Your Functions and Schemas: Create Python files with functions and Pydantic models for structured outputs.</li> <li>Leverage Integration with LangChain and Composio: Utilize tools available in Composio and LangChain.</li> <li>Visualize Your Project: Generate interactive graphical representations of your workflows.</li> <li>Execute the Workflow: GenSphere resolves dependencies and executes your workflow.</li> <li>Push to the Platform: Share your project on the open platform with a generated ID.</li> <li>Pull from the Platform: Use projects from others to build more complex applications.</li> <li>Watch Your Projects Grow: Monitor the popularity of your projects by tracking the number of pulls.</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation</li> <li>Quickstart Guide</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Quickstart Tutorial</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Workflows</li> <li>Functions and schemas</li> <li>Nesting workflows</li> <li>Integration with Composio and Langchain</li> <li>Visualization</li> <li>Execution</li> </ul>"},{"location":"#api-reference","title":"API reference","text":"<ul> <li>API reference</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please join our Discord server.</p> <p></p>"},{"location":"api_reference/api_reference/","title":"API Reference","text":"<p>This section provides detailed documentation of the core classes and utility functions used in GenSphere.</p>"},{"location":"api_reference/api_reference/#core-classes","title":"Core Classes","text":"<ul> <li>GenFlow</li> <li>Node</li> <li>YamlCompose</li> <li>Visualizer</li> <li>Hub</li> </ul>"},{"location":"api_reference/api_reference/#genflow","title":"GenFlow","text":"<p>Module: <code>genflow.py</code></p> <p>The <code>GenFlow</code> class is responsible for parsing YAML workflow definitions, constructing an execution graph, and executing nodes in the correct order. It manages the overall workflow execution process.</p>"},{"location":"api_reference/api_reference/#class-definition","title":"Class Definition","text":"<pre><code>class GenFlow:\n    def __init__(self, yaml_file, functions_filepath=None, structured_output_schema_filepath=None):\n        # Initialization code\n\n    def parse_yaml(self):\n        # Parses the YAML data and constructs nodes\n\n    def build_graph(self):\n        # Builds the execution graph\n\n    def run(self):\n        # Executes the nodes in topological order\n</code></pre>"},{"location":"api_reference/api_reference/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file, functions_filepath=None, structured_output_schema_filepath=None):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the main YAML file defining the workflow.</li> <li><code>functions_filepath</code> (str, optional): Path to the Python file containing custom function definitions.</li> <li><code>structured_output_schema_filepath</code> (str, optional): Path to the Python file containing structured output schemas.</li> </ul> <p>Description:</p> <p>Initializes the <code>GenFlow</code> instance by loading the YAML data and preparing the environment for execution. It verifies the validity of provided file paths and ensures that the necessary files are accessible.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the provided <code>functions_filepath</code> or <code>structured_output_schema_filepath</code> does not exist.</li> <li><code>ValueError</code>: If the provided file paths are not <code>.py</code> files.</li> </ul>"},{"location":"api_reference/api_reference/#methods","title":"Methods","text":""},{"location":"api_reference/api_reference/#parse_yaml","title":"<code>parse_yaml</code>","text":"<pre><code>def parse_yaml(self):\n</code></pre> <p>Description:</p> <p>Parses the YAML data from the main workflow file and constructs the nodes for execution. It also checks for the presence of nested workflows (<code>yml_flow</code> nodes) and composes them using <code>YamlCompose</code> if necessary. Validates the YAML file for consistency before parsing.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If the YAML file fails consistency checks.</li> </ul> <p>Example Usage:</p> <pre><code>flow = GenFlow('workflow.yaml', 'functions.py', 'schemas.py')\nflow.parse_yaml()\n</code></pre>"},{"location":"api_reference/api_reference/#build_graph","title":"<code>build_graph</code>","text":"<pre><code>def build_graph(self):\n</code></pre> <p>Description:</p> <p>Builds a directed acyclic graph (DAG) representing the execution order of nodes based on their dependencies. It adds nodes and edges to the graph according to the dependencies identified during parsing.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the graph contains cycles or if a node depends on an undefined node or variable.</li> </ul> <p>Example Usage:</p> <pre><code>flow.build_graph()\n</code></pre>"},{"location":"api_reference/api_reference/#run","title":"<code>run</code>","text":"<pre><code>def run(self):\n</code></pre> <p>Description:</p> <p>Executes the nodes in the order determined by the topological sort of the execution graph. It renders the parameters for each node using the outputs of previously executed nodes and handles iterative execution for nodes processing lists.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If there are cycles in the graph or if an error occurs during node execution.</li> </ul> <p>Example Usage:</p> <pre><code>flow.run()\n</code></pre> <p>After execution, the outputs from each node are stored in the <code>outputs</code> attribute of the <code>GenFlow</code> instance.</p>"},{"location":"api_reference/api_reference/#node","title":"Node","text":"<p>Module: <code>genflow.py</code></p> <p>The <code>Node</code> class represents an individual operation or step within the workflow. It encapsulates the logic required to execute that step, including parameter rendering and function execution.</p>"},{"location":"api_reference/api_reference/#class-definition_1","title":"Class Definition","text":"<pre><code>class Node:\n    def __init__(self, node_data):\n        # Initialization code\n\n    def set_flow(self, flow):\n        # Sets reference to the GenFlow instance\n\n    def get_dependencies(self, node_names):\n        # Retrieves the dependencies of the node\n\n    def render_params(self, outputs, env):\n        # Renders the parameters using previous outputs\n\n    def execute(self, params):\n        # Executes the node based on its type and parameters\n</code></pre>"},{"location":"api_reference/api_reference/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, node_data):\n</code></pre> <p>Parameters:</p> <ul> <li><code>node_data</code> (dict): Dictionary containing the node's configuration from the YAML file.</li> </ul> <p>Description:</p> <p>Initializes the <code>Node</code> instance with the given configuration. It extracts essential information such as the node's name, type, outputs, and parameters.</p>"},{"location":"api_reference/api_reference/#methods_1","title":"Methods","text":""},{"location":"api_reference/api_reference/#set_flow","title":"<code>set_flow</code>","text":"<pre><code>def set_flow(self, flow):\n</code></pre> <p>Parameters:</p> <ul> <li><code>flow</code> (GenFlow): Reference to the <code>GenFlow</code> instance managing the workflow execution.</li> </ul> <p>Description:</p> <p>Sets the reference to the <code>GenFlow</code> instance, allowing the node to access shared resources and configurations during execution.</p>"},{"location":"api_reference/api_reference/#get_dependencies","title":"<code>get_dependencies</code>","text":"<pre><code>def get_dependencies(self, node_names):\n</code></pre> <p>Parameters:</p> <ul> <li><code>node_names</code> (Iterable[str]): Iterable of all node names in the workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>dependencies</code> (Set[str]): Set of node names that the current node depends on.</li> </ul> <p>Description:</p> <p>Analyzes the node's parameters to determine which other nodes it depends on. This is used to build the execution graph and ensure correct execution order.</p> <p>Example Usage:</p> <pre><code>dependencies = node.get_dependencies(flow.nodes.keys())\n</code></pre>"},{"location":"api_reference/api_reference/#render_params","title":"<code>render_params</code>","text":"<pre><code>def render_params(self, outputs, env):\n</code></pre> <p>Parameters:</p> <ul> <li><code>outputs</code> (dict): Outputs from previously executed nodes.</li> <li><code>env</code> (jinja2.Environment): Jinja2 environment used for templating.</li> </ul> <p>Returns:</p> <ul> <li><code>rendered_params</code> (dict or list of dicts): Parameters with values rendered using the outputs of previous nodes.</li> </ul> <p>Description:</p> <p>Renders the node's parameters by substituting placeholders with actual values from previous outputs. Supports handling of indexed parameters and lists for iterative processing.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If a referenced variable is not found or is not iterable when expected.</li> </ul>"},{"location":"api_reference/api_reference/#execute","title":"<code>execute</code>","text":"<pre><code>def execute(self, params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters to be used for the node execution.</li> </ul> <p>Returns:</p> <ul> <li><code>outputs</code> (dict): Dictionary of outputs produced by the node execution.</li> </ul> <p>Description:</p> <p>Executes the node based on its type:</p> <ul> <li>For <code>function_call</code> nodes, it executes a Python function.</li> <li>For <code>llm_service</code> nodes, it interacts with an LLM service like OpenAI.</li> </ul> <p>Delegates to specific execution methods depending on the node type.</p> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: If the node type is not supported.</li> <li><code>Exception</code>: If an error occurs during execution.</li> </ul> <p>Example Usage:</p> <pre><code>outputs = node.execute(rendered_params)\n</code></pre>"},{"location":"api_reference/api_reference/#yamlcompose","title":"YamlCompose","text":"<p>Module: <code>yaml_utils.py</code></p> <p>The <code>YamlCompose</code> class is responsible for composing multiple YAML workflow files into a single unified workflow. It resolves references to nested workflows (<code>yml_flow</code> nodes) and adjusts node names and parameters to ensure uniqueness and consistency.</p>"},{"location":"api_reference/api_reference/#class-definition_2","title":"Class Definition","text":"<pre><code>class YamlCompose:\n    def __init__(self, yaml_file, functions_filepath, structured_output_schema_filepath):\n        # Initialization code\n\n    def compose(self, save_combined_yaml=False, output_file='combined.yaml'):\n        # Starts the composition process and returns the combined YAML data\n</code></pre>"},{"location":"api_reference/api_reference/#constructor_2","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file, functions_filepath, structured_output_schema_filepath):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the root YAML file to be composed.</li> <li><code>functions_filepath</code> (str): Path to the Python file containing custom functions.</li> <li><code>structured_output_schema_filepath</code> (str): Path to the Python file containing structured output schemas.</li> </ul> <p>Description:</p> <p>Initializes the <code>YamlCompose</code> instance and prepares for the composition process by validating the provided file paths.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the provided file paths do not exist.</li> <li><code>ValueError</code>: If the provided file paths are not <code>.py</code> files.</li> </ul>"},{"location":"api_reference/api_reference/#methods_2","title":"Methods","text":""},{"location":"api_reference/api_reference/#compose","title":"<code>compose</code>","text":"<pre><code>def compose(self, save_combined_yaml=False, output_file='combined.yaml'):\n</code></pre> <p>Parameters:</p> <ul> <li><code>save_combined_yaml</code> (bool, optional): If <code>True</code>, saves the combined YAML data to a file.</li> <li><code>output_file</code> (str, optional): Filename to save the combined YAML data.</li> </ul> <p>Returns:</p> <ul> <li><code>combined_data</code> (dict): The combined YAML data after composition.</li> </ul> <p>Description:</p> <p>Starts the composition process by recursively processing the root YAML file and any nested sub-flows. Adjusts node names and parameter references to ensure uniqueness across the combined workflow.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If validation fails during composition.</li> </ul> <p>Example Usage:</p> <pre><code>composer = YamlCompose('main_workflow.yaml', 'functions.py', 'schemas.py')\ncombined_yaml_data = composer.compose(save_combined_yaml=True, output_file='combined.yaml')\n</code></pre> <p>After composition, the combined YAML file can be executed as a single workflow.</p>"},{"location":"api_reference/api_reference/#visualizer","title":"Visualizer","text":"<p>Module: <code>visualizer.py</code></p> <p>The <code>Visualizer</code> class provides a graphical representation of GenSphere workflows using a web-based interface powered by Dash and Cytoscape. It allows users to visualize nodes, their types, dependencies, and inspect details of each node interactively.</p>"},{"location":"api_reference/api_reference/#class-definition_3","title":"Class Definition","text":"<pre><code>class Visualizer:\n    def __init__(self, yaml_file=None, functions_filepath=None, structured_output_schema_filepath=None, address='127.0.0.1', port=8050):\n        # Initialization code\n\n    def start_visualization(self):\n        # Starts the Dash application for visualization\n</code></pre>"},{"location":"api_reference/api_reference/#constructor_3","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file=None, functions_filepath=None, structured_output_schema_filepath=None, address='127.0.0.1', port=8050):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str, optional): Path to the YAML file defining the workflow.</li> <li><code>functions_filepath</code> (str, optional): Path to the Python file containing custom function definitions.</li> <li><code>structured_output_schema_filepath</code> (str, optional): Path to the Python file containing structured output schemas.</li> <li><code>address</code> (str, optional): The IP address to host the Dash app (default: <code>'127.0.0.1'</code>).</li> <li><code>port</code> (int, optional): The port to host the Dash app (default: <code>8050</code>).</li> </ul> <p>Description:</p> <p>Initializes the <code>Visualizer</code> instance by setting up the necessary file paths and loading the user-provided functions and schemas. It validates the existence and correctness of the provided files and prepares the environment for visualization.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If any of the provided file paths do not exist.</li> <li><code>ValueError</code>: If the provided files are not <code>.py</code> files.</li> </ul> <p>Example Usage:</p> <pre><code>from gensphere.visualizer import Visualizer\n\nviz = Visualizer(\n    yaml_file='workflow.yaml',\n    functions_filepath='functions.py',\n    structured_output_schema_filepath='schemas.py',\n    address='127.0.0.1',\n    port=8050\n)\n</code></pre>"},{"location":"api_reference/api_reference/#methods_3","title":"Methods","text":""},{"location":"api_reference/api_reference/#start_visualization","title":"<code>start_visualization</code>","text":"<pre><code>def start_visualization(self):\n</code></pre> <p>Description:</p> <p>Starts the Dash application for visualizing the GenSphere workflow. The application provides an interactive interface where nodes are displayed graphically, and users can click on nodes to view detailed information such as parameters, outputs, functions, and schemas.</p> <p>Features:</p> <ul> <li>Graph Visualization: Uses Cytoscape to render the workflow graph.</li> <li>Interactive Nodes: Clicking on a node displays detailed information.</li> <li>Legend: Includes a legend explaining node types and edge styles.</li> <li>Dynamic Loading: Users can input a different YAML file path and reload the graph.</li> </ul> <p>Example Usage:</p> <pre><code>viz.start_visualization()\n</code></pre> <p>After running this method, navigate to <code>http://127.0.0.1:8050</code> in your web browser to view the visualization.</p> <p>Notes:</p> <ul> <li>Ensure that the YAML file and any referenced functions or schemas are correctly specified.</li> <li>The visualization runs a local web server; make sure the specified <code>address</code> and <code>port</code> are accessible.</li> </ul>"},{"location":"api_reference/api_reference/#hub","title":"Hub","text":"<p>Module: <code>hub.py</code></p> <p>The <code>Hub</code> class provides an interface to interact with the GenSphere Hub platform. It allows users to push workflows to the hub, pull workflows from the hub, and check the number of times a workflow has been pulled.</p>"},{"location":"api_reference/api_reference/#class-definition_4","title":"Class Definition","text":"<pre><code>class Hub:\n    def __init__(self, yaml_file=None, functions_file=None, schema_file=None, api_base_url='http://genspherehub.us-east-1.elasticbeanstalk.com/'):\n        # Initialization code\n\n    def push(self, push_name=None):\n        # Pushes the workflow to the GenSphere Hub\n\n    def pull(self, push_id, save_to_disk=True, yaml_filename=None, functions_filename=None, schema_filename=None, download_path=\".\"):\n        # Pulls a workflow from the GenSphere Hub\n\n    def count_pulls(self, push_id):\n        # Retrieves the total number of times a push has been pulled\n</code></pre>"},{"location":"api_reference/api_reference/#constructor_4","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file=None, functions_file=None, schema_file=None, api_base_url='http://genspherehub.us-east-1.elasticbeanstalk.com/'):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str, optional): Path to the YAML file to be pushed.</li> <li><code>functions_file</code> (str, optional): Path to the functions file to be pushed.</li> <li><code>schema_file</code> (str, optional): Path to the schema file to be pushed.</li> <li><code>api_base_url</code> (str, optional): Base URL for the GenSphere Hub API.</li> </ul> <p>Description:</p> <p>Initializes the <code>Hub</code> instance with the provided file paths and API base URL. Prepares the instance for pushing and pulling workflows to and from the GenSphere Hub platform.</p> <p>Example Usage:</p> <pre><code>from gensphere.hub import Hub\n\nhub = Hub(\n    yaml_file='workflow.yaml',\n    functions_file='functions.py',\n    schema_file='schemas.py'\n)\n</code></pre>"},{"location":"api_reference/api_reference/#methods_4","title":"Methods","text":""},{"location":"api_reference/api_reference/#push","title":"<code>push</code>","text":"<pre><code>def push(self, push_name=None):\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_name</code> (str, optional): A descriptive name for the workflow being pushed.</li> </ul> <p>Returns:</p> <ul> <li><code>result</code> (dict): A dictionary containing the <code>push_id</code> and a list of uploaded files.</li> </ul> <p>Description:</p> <p>Pushes the specified workflow files to the GenSphere Hub. Validates the YAML file for consistency before pushing. The <code>push_id</code> returned can be used to pull the workflow or check its pull count.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If no <code>yaml_file</code> is provided or if the functions or schema files are not <code>.py</code> files.</li> <li><code>Exception</code>: If validation fails or if an error occurs during the push.</li> </ul> <p>Example Usage:</p> <pre><code>result = hub.push(push_name='My Awesome Workflow')\npush_id = result.get('push_id')\nprint(f\"Workflow pushed with push_id: {push_id}\")\n</code></pre>"},{"location":"api_reference/api_reference/#pull","title":"<code>pull</code>","text":"<pre><code>def pull(self, push_id, save_to_disk=True, yaml_filename=None, functions_filename=None, schema_filename=None, download_path=\".\"):\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_id</code> (str): The <code>push_id</code> of the workflow to pull.</li> <li><code>save_to_disk</code> (bool, optional): If <code>True</code>, saves the pulled files to disk (default: <code>True</code>).</li> <li><code>yaml_filename</code> (str, optional): Custom filename for the YAML file.</li> <li><code>functions_filename</code> (str, optional): Custom filename for the functions file.</li> <li><code>schema_filename</code> (str, optional): Custom filename for the schema file.</li> <li><code>download_path</code> (str, optional): Directory to save the pulled files (default: <code>\".\"</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>files_content</code> (dict): A dictionary containing the contents of the pulled files.</li> </ul> <p>Description:</p> <p>Pulls a workflow from the GenSphere Hub using the provided <code>push_id</code>. Optionally saves the files to disk with custom filenames. Ensures that existing files are not overwritten by appending a counter if necessary.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If an error occurs during the pull operation.</li> </ul> <p>Example Usage:</p> <pre><code>files = hub.pull(\n    push_id=push_id,\n    save_to_disk=True,\n    yaml_filename='downloaded_workflow.yaml',\n    functions_filename='downloaded_functions.py',\n    schema_filename='downloaded_schemas.py'\n)\n</code></pre>"},{"location":"api_reference/api_reference/#count_pulls","title":"<code>count_pulls</code>","text":"<pre><code>def count_pulls(self, push_id):\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_id</code> (str): The <code>push_id</code> of the workflow to check.</li> </ul> <p>Returns:</p> <ul> <li><code>pull_count</code> (int): The total number of times the workflow has been pulled.</li> </ul> <p>Description:</p> <p>Retrieves the total number of times a workflow has been pulled from the GenSphere Hub using the provided <code>push_id</code>.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If an error occurs during the request.</li> </ul> <p>Example Usage:</p> <pre><code>pull_count = hub.count_pulls(push_id=push_id)\nprint(f\"The workflow has been pulled {pull_count} times.\")\n</code></pre>"},{"location":"api_reference/api_reference/#utility-functions","title":"Utility Functions","text":"<p>This section documents the utility functions used within GenSphere, primarily for internal processing and validation.</p>"},{"location":"api_reference/api_reference/#get_function_schema","title":"get_function_schema","text":"<p>Module: <code>genflow.py</code></p> <pre><code>def get_function_schema(func):\n</code></pre> <p>Parameters:</p> <ul> <li><code>func</code> (function): The Python function object to generate a schema for.</li> </ul> <p>Returns:</p> <ul> <li><code>function_def</code> (dict): A dictionary representing the function definition, including name, description, and parameters.</li> </ul> <p>Description:</p> <p>Generates a schema for a given function by inspecting its signature and docstring. This schema is used for OpenAI's function calling feature in LLM service nodes. It ensures that the function parameters are properly typed and documented.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If a parameter lacks a type annotation or if the function lacks a docstring.</li> </ul> <p>Example Usage:</p> <p>Used internally when preparing function definitions for OpenAI's function calling.</p>"},{"location":"api_reference/api_reference/#validate_yaml","title":"validate_yaml","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def validate_yaml(\n    yaml_file,\n    functions_filepath=None,\n    structured_output_schema_filepath=None,\n    parent_node_names=None,\n    visited_files=None,\n    parent_params=None,\n    parent_node_outputs=None\n):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file being validated.</li> <li><code>functions_filepath</code> (str, optional): Path to the functions file.</li> <li><code>structured_output_schema_filepath</code> (str, optional): Path to the schemas file.</li> <li><code>parent_node_names</code> (Set[str], optional): Set of node names from the parent flow.</li> <li><code>visited_files</code> (Set[str], optional): Set of visited YAML files to prevent circular references.</li> <li><code>parent_params</code> (Set[str], optional): Set of parameter names passed from the parent flow.</li> <li><code>parent_node_outputs</code> (Dict[str, List[str]], optional): Dictionary of node outputs from parent flows.</li> </ul> <p>Returns:</p> <ul> <li><code>validated</code> (bool): <code>True</code> if validation passes, <code>False</code> otherwise.</li> <li><code>error_msgs</code> (List[str]): List of error messages encountered during validation.</li> <li><code>node_outputs</code> (Dict[str, List[str]]): Dictionary of node outputs in the current flow.</li> </ul> <p>Description:</p> <p>Validates a YAML workflow file and any associated sub-flows for consistency and correctness. Checks for issues such as:</p> <ul> <li>Missing required fields (<code>name</code>, <code>type</code>).</li> <li>Duplicate node names.</li> <li>Undefined or duplicate outputs.</li> <li>References to undefined nodes or outputs.</li> <li>Cycles in the execution graph.</li> <li>Validity of functions and schemas.</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If referenced files do not exist.</li> <li><code>ValueError</code>: If the YAML structure is invalid.</li> </ul> <p>Example Usage:</p> <p>Used internally before executing or composing workflows to ensure they are valid.</p>"},{"location":"api_reference/api_reference/#collect_referenced_nodes_and_outputs","title":"collect_referenced_nodes_and_outputs","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_referenced_nodes_and_outputs(params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters dictionary from a node.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes_outputs</code> (Set[Tuple[str, str]]): A set of tuples containing referenced node names and outputs.</li> </ul> <p>Description:</p> <p>Analyzes the parameters of a node to identify all referenced nodes and their outputs, which is essential for validating dependencies and ensuring that all references are valid.</p>"},{"location":"api_reference/api_reference/#collect_used_params","title":"collect_used_params","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_used_params(yaml_data):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_data</code> (dict): The YAML data of a workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>used_params</code> (Set[str]): A set of parameter names used within the workflow.</li> </ul> <p>Description:</p> <p>Collects all parameter names that are used in the workflow, particularly in the context of nested workflows (<code>yml_flow</code> nodes). This helps in validating that all required parameters are provided.</p>"},{"location":"api_reference/api_reference/#collect_referenced_params","title":"collect_referenced_params","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_referenced_params(params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters dictionary from a node.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_params</code> (Set[str]): A set of parameter names referenced in the parameters.</li> </ul> <p>Description:</p> <p>Identifies all parameter names that are referenced within the node's parameters, usually in templated strings. This is used to ensure that all referenced parameters are defined.</p>"},{"location":"api_reference/api_reference/#collect_referenced_nodes","title":"collect_referenced_nodes","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_referenced_nodes(params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters dictionary from a node.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes</code> (Set[str]): A set of node names referenced in the parameters.</li> </ul> <p>Description:</p> <p>Identifies all node names that are referenced within the node's parameters. This is crucial for building the execution graph and determining the correct execution order.</p>"},{"location":"api_reference/api_reference/#load_yaml_file","title":"load_yaml_file","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def load_yaml_file(yaml_file):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file to load.</li> </ul> <p>Returns:</p> <ul> <li><code>data</code> (dict): The loaded YAML data.</li> </ul> <p>Description:</p> <p>Loads the YAML data from a file and handles parsing errors. Ensures that the file exists and contains valid YAML.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the YAML file does not exist.</li> <li><code>ValueError</code>: If there is an error parsing the YAML file.</li> </ul>"},{"location":"api_reference/api_reference/#has_yml_flow_nodes","title":"has_yml_flow_nodes","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def has_yml_flow_nodes(yaml_data):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_data</code> (dict): The YAML data of a workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: <code>True</code> if the workflow contains any <code>yml_flow</code> nodes, <code>False</code> otherwise.</li> </ul> <p>Description:</p> <p>Checks whether the given YAML data contains any nested workflows (<code>yml_flow</code> nodes). This helps determine if composition is necessary before execution.</p>"},{"location":"api_reference/api_reference/#get_base_output_name","title":"get_base_output_name","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def get_base_output_name(output_reference):\n</code></pre> <p>Parameters:</p> <ul> <li><code>output_reference</code> (str): A string representing an output reference (e.g., <code>'countries_list[i]'</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>base_output_name</code> (str): The base output name extracted from the reference.</li> </ul> <p>Description:</p> <p>Extracts the base output name from a complex output reference that may include indexing or attribute access. Used during validation to identify the actual outputs being referenced.</p>"},{"location":"api_reference/api_reference/#parse_yaml_1","title":"parse_yaml","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def parse_yaml(yaml_file):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file to parse.</li> </ul> <p>Returns:</p> <ul> <li><code>data</code> (dict): Parsed YAML data.</li> </ul> <p>Description:</p> <p>Parses a YAML file and returns its content as a dictionary. Validates the existence of the file and handles parsing errors.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the YAML file does not exist.</li> <li><code>yaml.YAMLError</code>: If an error occurs during YAML parsing.</li> </ul> <p>Example Usage:</p> <pre><code>data = parse_yaml('workflow.yaml')\n</code></pre>"},{"location":"api_reference/api_reference/#extract_referenced_nodes","title":"extract_referenced_nodes","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def extract_referenced_nodes(template_str):\n</code></pre> <p>Parameters:</p> <ul> <li><code>template_str</code> (str): A templated string containing references to other nodes (e.g., <code>\"{{ node.output }}\"</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes</code> (Set[str]): A set of referenced node names.</li> </ul> <p>Description:</p> <p>Extracts all referenced node names from a templated string using regular expressions. Useful for identifying dependencies between nodes in a workflow.</p> <p>Example Usage:</p> <pre><code>template_str = \"{{ node1.output }} and {{ node2.output }}\"\nreferenced_nodes = extract_referenced_nodes(template_str)\n# referenced_nodes will be {'node1', 'node2'}\n</code></pre>"},{"location":"api_reference/api_reference/#traverse_node_fields","title":"traverse_node_fields","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def traverse_node_fields(node_value):\n</code></pre> <p>Parameters:</p> <ul> <li><code>node_value</code> (Union[str, dict, list]): The node value to traverse.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes</code> (Set[str]): A set of referenced node names found within the node value.</li> </ul> <p>Description:</p> <p>Recursively traverses a node's fields to find all referenced node names. Handles strings, dictionaries, and lists. Used to identify all dependencies for a node.</p> <p>Example Usage:</p> <pre><code>node_params = {\n    'param1': '{{ node1.output }}',\n    'param2': {\n        'subparam': '{{ node2.output }}'\n    }\n}\nreferenced_nodes = traverse_node_fields(node_params)\n# referenced_nodes will be {'node1', 'node2'}\n</code></pre>"},{"location":"api_reference/api_reference/#identify_and_style_entrypoints_outputs","title":"identify_and_style_entrypoints_outputs","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def identify_and_style_entrypoints_outputs(elements):\n</code></pre> <p>Parameters:</p> <ul> <li><code>elements</code> (list): List of Cytoscape elements (nodes and edges).</li> </ul> <p>Returns:</p> <ul> <li><code>elements</code> (list): Updated list of Cytoscape elements with styled entrypoints and output nodes.</li> </ul> <p>Description:</p> <p>Identifies entrypoint nodes (nodes with no incoming edges) and output nodes (nodes with no outgoing edges) in the workflow graph and styles them accordingly for visualization purposes.</p> <p>Example Usage:</p> <pre><code>elements = identify_and_style_entrypoints_outputs(elements)\n</code></pre>"},{"location":"api_reference/api_reference/#build_graph_data","title":"build_graph_data","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def build_graph_data(yaml_file):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file defining the workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>elements</code> (list): List of Cytoscape elements (nodes and edges) representing the workflow graph.</li> </ul> <p>Description:</p> <p>Builds graph data compatible with Cytoscape from a YAML workflow definition. It processes nodes and edges, identifies dependencies, and prepares the data for visualization.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If a node lacks a name or if there are duplicate node names.</li> </ul> <p>Example Usage:</p> <pre><code>elements = build_graph_data('workflow.yaml')\n</code></pre>"},{"location":"api_reference/api_reference/#additional-information","title":"Additional Information","text":"<p>These utility functions are primarily used internally by GenSphere to process and validate workflows. Understanding them can be helpful for advanced users who wish to extend or debug the framework.</p> <p>Note: When developing custom functions or schemas for use in GenSphere workflows, ensure that:</p> <ul> <li>Functions have proper docstrings and type annotations.</li> <li>Schemas are defined using Pydantic models.</li> <li>Functions and schemas are placed in the files specified when initializing <code>GenFlow</code> or <code>YamlCompose</code>.</li> </ul>"},{"location":"api_reference/api_reference/#conclusion","title":"Conclusion","text":"<p>For more examples and usage instructions, refer to the  Tutorials.</p> <p>If you have any questions or need further assistance, reach out on our GitHub Issues page.</p>"},{"location":"api_reference/api_reference2/","title":"API Reference","text":"<p>This section provides detailed documentation of the core classes and utility functions used in GenSphere, specifically focusing on the <code>visualizer.py</code>, <code>graph_builder.py</code>, and <code>hub.py</code> modules.</p>"},{"location":"api_reference/api_reference2/#core-classes","title":"Core Classes","text":"<ul> <li>Visualizer</li> <li>Hub</li> </ul>"},{"location":"api_reference/api_reference2/#utility-functions","title":"Utility Functions","text":"<ul> <li>parse_yaml</li> <li>extract_referenced_nodes</li> <li>traverse_node_fields</li> <li>identify_and_style_entrypoints_outputs</li> <li>build_graph_data</li> </ul>"},{"location":"api_reference/api_reference2/#visualizer","title":"Visualizer","text":"<p>Module: <code>visualizer.py</code></p> <p>The <code>Visualizer</code> class provides a graphical representation of GenSphere workflows using a web-based interface powered by Dash and Cytoscape. It allows users to visualize nodes, their types, dependencies, and inspect details of each node interactively.</p>"},{"location":"api_reference/api_reference2/#class-definition","title":"Class Definition","text":"<pre><code>class Visualizer:\n    def __init__(self, yaml_file=None, functions_filepath=None, structured_output_schema_filepath=None, address='127.0.0.1', port=8050):\n        # Initialization code\n\n    def start_visualization(self):\n        # Starts the Dash application for visualization\n</code></pre>"},{"location":"api_reference/api_reference2/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file=None, functions_filepath=None, structured_output_schema_filepath=None, address='127.0.0.1', port=8050):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str, optional): Path to the YAML file defining the workflow.</li> <li><code>functions_filepath</code> (str, optional): Path to the Python file containing custom function definitions.</li> <li><code>structured_output_schema_filepath</code> (str, optional): Path to the Python file containing structured output schemas.</li> <li><code>address</code> (str, optional): The IP address to host the Dash app (default: <code>'127.0.0.1'</code>).</li> <li><code>port</code> (int, optional): The port to host the Dash app (default: <code>8050</code>).</li> </ul> <p>Description:</p> <p>Initializes the <code>Visualizer</code> instance by setting up the necessary file paths and loading the user-provided functions and schemas. It validates the existence and correctness of the provided files and prepares the environment for visualization.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If any of the provided file paths do not exist.</li> <li><code>ValueError</code>: If the provided files are not <code>.py</code> files.</li> </ul> <p>Example Usage:</p> <pre><code>from gensphere.visualizer import Visualizer\n\nviz = Visualizer(\n    yaml_file='workflow.yaml',\n    functions_filepath='functions.py',\n    structured_output_schema_filepath='schemas.py',\n    address='127.0.0.1',\n    port=8050\n)\n</code></pre>"},{"location":"api_reference/api_reference2/#methods","title":"Methods","text":""},{"location":"api_reference/api_reference2/#start_visualization","title":"<code>start_visualization</code>","text":"<pre><code>def start_visualization(self):\n</code></pre> <p>Description:</p> <p>Starts the Dash application for visualizing the GenSphere workflow. The application provides an interactive interface where nodes are displayed graphically, and users can click on nodes to view detailed information such as parameters, outputs, functions, and schemas.</p> <p>Features:</p> <ul> <li>Graph Visualization: Uses Cytoscape to render the workflow graph.</li> <li>Interactive Nodes: Clicking on a node displays detailed information.</li> <li>Legend: Includes a legend explaining node types and edge styles.</li> <li>Dynamic Loading: Users can input a different YAML file path and reload the graph.</li> </ul> <p>Example Usage:</p> <pre><code>viz.start_visualization()\n</code></pre> <p>After running this method, navigate to <code>http://127.0.0.1:8050</code> in your web browser to view the visualization.</p> <p>Notes:</p> <ul> <li>Ensure that the YAML file and any referenced functions or schemas are correctly specified.</li> <li>The visualization runs a local web server; make sure the specified <code>address</code> and <code>port</code> are accessible.</li> </ul>"},{"location":"api_reference/api_reference2/#hub","title":"Hub","text":"<p>Module: <code>hub.py</code></p> <p>The <code>Hub</code> class provides an interface to interact with the GenSphere Hub platform. It allows users to push workflows to the hub, pull workflows from the hub, and check the number of times a workflow has been pulled.</p>"},{"location":"api_reference/api_reference2/#class-definition_1","title":"Class Definition","text":"<pre><code>class Hub:\n    def __init__(self, yaml_file=None, functions_file=None, schema_file=None, api_base_url='http://genspherehub.us-east-1.elasticbeanstalk.com/'):\n        # Initialization code\n\n    def push(self, push_name=None):\n        # Pushes the workflow to the GenSphere Hub\n\n    def pull(self, push_id, save_to_disk=True, yaml_filename=None, functions_filename=None, schema_filename=None, download_path=\".\"):\n        # Pulls a workflow from the GenSphere Hub\n\n    def count_pulls(self, push_id):\n        # Retrieves the total number of times a push has been pulled\n</code></pre>"},{"location":"api_reference/api_reference2/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file=None, functions_file=None, schema_file=None, api_base_url='http://genspherehub.us-east-1.elasticbeanstalk.com/'):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str, optional): Path to the YAML file to be pushed.</li> <li><code>functions_file</code> (str, optional): Path to the functions file to be pushed.</li> <li><code>schema_file</code> (str, optional): Path to the schema file to be pushed.</li> <li><code>api_base_url</code> (str, optional): Base URL for the GenSphere Hub API.</li> </ul> <p>Description:</p> <p>Initializes the <code>Hub</code> instance with the provided file paths and API base URL. Prepares the instance for pushing and pulling workflows to and from the GenSphere Hub platform.</p> <p>Example Usage:</p> <pre><code>from gensphere.hub import Hub\n\nhub = Hub(\n    yaml_file='workflow.yaml',\n    functions_file='functions.py',\n    schema_file='schemas.py'\n)\n</code></pre>"},{"location":"api_reference/api_reference2/#methods_1","title":"Methods","text":""},{"location":"api_reference/api_reference2/#push","title":"<code>push</code>","text":"<pre><code>def push(self, push_name=None):\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_name</code> (str, optional): A descriptive name for the workflow being pushed.</li> </ul> <p>Returns:</p> <ul> <li><code>result</code> (dict): A dictionary containing the <code>push_id</code> and a list of uploaded files.</li> </ul> <p>Description:</p> <p>Pushes the specified workflow files to the GenSphere Hub. Validates the YAML file for consistency before pushing. The <code>push_id</code> returned can be used to pull the workflow or check its pull count.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If no <code>yaml_file</code> is provided or if the functions or schema files are not <code>.py</code> files.</li> <li><code>Exception</code>: If validation fails or if an error occurs during the push.</li> </ul> <p>Example Usage:</p> <pre><code>result = hub.push(push_name='My Awesome Workflow')\npush_id = result.get('push_id')\nprint(f\"Workflow pushed with push_id: {push_id}\")\n</code></pre>"},{"location":"api_reference/api_reference2/#pull","title":"<code>pull</code>","text":"<pre><code>def pull(self, push_id, save_to_disk=True, yaml_filename=None, functions_filename=None, schema_filename=None, download_path=\".\"):\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_id</code> (str): The <code>push_id</code> of the workflow to pull.</li> <li><code>save_to_disk</code> (bool, optional): If <code>True</code>, saves the pulled files to disk (default: <code>True</code>).</li> <li><code>yaml_filename</code> (str, optional): Custom filename for the YAML file.</li> <li><code>functions_filename</code> (str, optional): Custom filename for the functions file.</li> <li><code>schema_filename</code> (str, optional): Custom filename for the schema file.</li> <li><code>download_path</code> (str, optional): Directory to save the pulled files (default: <code>\".\"</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>files_content</code> (dict): A dictionary containing the contents of the pulled files.</li> </ul> <p>Description:</p> <p>Pulls a workflow from the GenSphere Hub using the provided <code>push_id</code>. Optionally saves the files to disk with custom filenames. Ensures that existing files are not overwritten by appending a counter if necessary.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If an error occurs during the pull operation.</li> </ul> <p>Example Usage:</p> <pre><code>files = hub.pull(\n    push_id=push_id,\n    save_to_disk=True,\n    yaml_filename='downloaded_workflow.yaml',\n    functions_filename='downloaded_functions.py',\n    schema_filename='downloaded_schemas.py'\n)\n</code></pre>"},{"location":"api_reference/api_reference2/#count_pulls","title":"<code>count_pulls</code>","text":"<pre><code>def count_pulls(self, push_id):\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_id</code> (str): The <code>push_id</code> of the workflow to check.</li> </ul> <p>Returns:</p> <ul> <li><code>pull_count</code> (int): The total number of times the workflow has been pulled.</li> </ul> <p>Description:</p> <p>Retrieves the total number of times a workflow has been pulled from the GenSphere Hub using the provided <code>push_id</code>.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If an error occurs during the request.</li> </ul> <p>Example Usage:</p> <pre><code>pull_count = hub.count_pulls(push_id=push_id)\nprint(f\"The workflow has been pulled {pull_count} times.\")\n</code></pre>"},{"location":"api_reference/api_reference2/#utility-functions_1","title":"Utility Functions","text":""},{"location":"api_reference/api_reference2/#parse_yaml","title":"parse_yaml","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def parse_yaml(yaml_file):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file to parse.</li> </ul> <p>Returns:</p> <ul> <li><code>data</code> (dict): Parsed YAML data.</li> </ul> <p>Description:</p> <p>Parses a YAML file and returns its content as a dictionary. Validates the existence of the file and handles parsing errors.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the YAML file does not exist.</li> <li><code>yaml.YAMLError</code>: If an error occurs during YAML parsing.</li> </ul> <p>Example Usage:</p> <pre><code>data = parse_yaml('workflow.yaml')\n</code></pre>"},{"location":"api_reference/api_reference2/#extract_referenced_nodes","title":"extract_referenced_nodes","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def extract_referenced_nodes(template_str):\n</code></pre> <p>Parameters:</p> <ul> <li><code>template_str</code> (str): A templated string containing references to other nodes (e.g., <code>\"{{ node.output }}\"</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes</code> (Set[str]): A set of referenced node names.</li> </ul> <p>Description:</p> <p>Extracts all referenced node names from a templated string using regular expressions. Useful for identifying dependencies between nodes in a workflow.</p> <p>Example Usage:</p> <pre><code>template_str = \"{{ node1.output }} and {{ node2.output }}\"\nreferenced_nodes = extract_referenced_nodes(template_str)\n# referenced_nodes will be {'node1', 'node2'}\n</code></pre>"},{"location":"api_reference/api_reference2/#traverse_node_fields","title":"traverse_node_fields","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def traverse_node_fields(node_value):\n</code></pre> <p>Parameters:</p> <ul> <li><code>node_value</code> (Union[str, dict, list]): The node value to traverse.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes</code> (Set[str]): A set of referenced node names found within the node value.</li> </ul> <p>Description:</p> <p>Recursively traverses a node's fields to find all referenced node names. Handles strings, dictionaries, and lists. Used to identify all dependencies for a node.</p> <p>Example Usage:</p> <pre><code>node_params = {\n    'param1': '{{ node1.output }}',\n    'param2': {\n        'subparam': '{{ node2.output }}'\n    }\n}\nreferenced_nodes = traverse_node_fields(node_params)\n# referenced_nodes will be {'node1', 'node2'}\n</code></pre>"},{"location":"api_reference/api_reference2/#identify_and_style_entrypoints_outputs","title":"identify_and_style_entrypoints_outputs","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def identify_and_style_entrypoints_outputs(elements):\n</code></pre> <p>Parameters:</p> <ul> <li><code>elements</code> (list): List of Cytoscape elements (nodes and edges).</li> </ul> <p>Returns:</p> <ul> <li><code>elements</code> (list): Updated list of Cytoscape elements with styled entrypoints and output nodes.</li> </ul> <p>Description:</p> <p>Identifies entrypoint nodes (nodes with no incoming edges) and output nodes (nodes with no outgoing edges) in the workflow graph and styles them accordingly for visualization purposes.</p> <p>Example Usage:</p> <pre><code>elements = identify_and_style_entrypoints_outputs(elements)\n</code></pre>"},{"location":"api_reference/api_reference2/#build_graph_data","title":"build_graph_data","text":"<p>Module: <code>graph_builder.py</code></p> <pre><code>def build_graph_data(yaml_file):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file defining the workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>elements</code> (list): List of Cytoscape elements (nodes and edges) representing the workflow graph.</li> </ul> <p>Description:</p> <p>Builds graph data compatible with Cytoscape from a YAML workflow definition. It processes nodes and edges, identifies dependencies, and prepares the data for visualization.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If a node lacks a name or if there are duplicate node names.</li> </ul> <p>Example Usage:</p> <pre><code>elements = build_graph_data('workflow.yaml')\n</code></pre>"},{"location":"api_reference/api_reference2/#additional-information","title":"Additional Information","text":"<p>These classes and functions are integral to GenSphere's capabilities for visualizing workflows and interacting with the GenSphere Hub platform. Understanding them allows users to extend functionality, troubleshoot issues, and fully leverage GenSphere's features.</p> <p>Note: When using the <code>Visualizer</code> and <code>Hub</code> classes:</p> <ul> <li>Ensure that all file paths provided are correct and that files exist.</li> <li>Handle exceptions appropriately, especially when dealing with network requests in the <code>Hub</code> class.</li> </ul>"},{"location":"api_reference/api_reference2/#conclusion","title":"Conclusion","text":"<p>This API reference provides detailed insights into the classes and functions used for visualizing GenSphere workflows and interacting with the GenSphere Hub. With this knowledge, you can:</p> <ul> <li>Visualize your workflows to better understand execution flow and dependencies.</li> <li>Share and retrieve workflows from the GenSphere Hub platform.</li> <li>Extend and customize visualization and hub interactions according to your needs.</li> </ul> <p>For more examples and usage instructions, refer to the User Guide and Tutorials.</p> <p>If you have any questions or need further assistance, please refer to the FAQ or reach out on our GitHub Issues page.</p>"},{"location":"api_reference/api_reference2/#end-of-api-reference-sections","title":"End of API Reference Sections","text":"<p>You're now equipped with detailed knowledge of GenSphere's visualization and hub capabilities. Happy coding!</p>"},{"location":"api_reference/api_reference_/","title":"API Reference","text":"<p>This section provides detailed documentation of the core classes and utility functions used in GenSphere, specifically focusing on the <code>genflow.py</code> and <code>yaml_utils.py</code> modules.</p>"},{"location":"api_reference/api_reference_/#core-classes","title":"Core Classes","text":"<ul> <li>GenFlow</li> <li>Node</li> <li>YamlCompose</li> </ul>"},{"location":"api_reference/api_reference_/#genflow","title":"GenFlow","text":"<p>Module: <code>genflow.py</code></p> <p>The <code>GenFlow</code> class is responsible for parsing YAML workflow definitions, constructing an execution graph, and executing nodes in the correct order. It manages the overall workflow execution process.</p>"},{"location":"api_reference/api_reference_/#class-definition","title":"Class Definition","text":"<pre><code>class GenFlow:\n    def __init__(self, yaml_file, functions_filepath=None, structured_output_schema_filepath=None):\n        # Initialization code\n\n    def parse_yaml(self):\n        # Parses the YAML data and constructs nodes\n\n    def build_graph(self):\n        # Builds the execution graph\n\n    def run(self):\n        # Executes the nodes in topological order\n</code></pre>"},{"location":"api_reference/api_reference_/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file, functions_filepath=None, structured_output_schema_filepath=None):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the main YAML file defining the workflow.</li> <li><code>functions_filepath</code> (str, optional): Path to the Python file containing custom function definitions.</li> <li><code>structured_output_schema_filepath</code> (str, optional): Path to the Python file containing structured output schemas.</li> </ul> <p>Description:</p> <p>Initializes the <code>GenFlow</code> instance by loading the YAML data and preparing the environment for execution. It verifies the validity of provided file paths and ensures that the necessary files are accessible.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the provided <code>functions_filepath</code> or <code>structured_output_schema_filepath</code> does not exist.</li> <li><code>ValueError</code>: If the provided file paths are not <code>.py</code> files.</li> </ul>"},{"location":"api_reference/api_reference_/#methods","title":"Methods","text":""},{"location":"api_reference/api_reference_/#parse_yaml","title":"<code>parse_yaml</code>","text":"<pre><code>def parse_yaml(self):\n</code></pre> <p>Description:</p> <p>Parses the YAML data from the main workflow file and constructs the nodes for execution. It also checks for the presence of nested workflows (<code>yml_flow</code> nodes) and composes them using <code>YamlCompose</code> if necessary. Validates the YAML file for consistency before parsing.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If the YAML file fails consistency checks.</li> </ul> <p>Example Usage:</p> <pre><code>flow = GenFlow('workflow.yaml', 'functions.py', 'schemas.py')\nflow.parse_yaml()\n</code></pre>"},{"location":"api_reference/api_reference_/#build_graph","title":"<code>build_graph</code>","text":"<pre><code>def build_graph(self):\n</code></pre> <p>Description:</p> <p>Builds a directed acyclic graph (DAG) representing the execution order of nodes based on their dependencies. It adds nodes and edges to the graph according to the dependencies identified during parsing.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the graph contains cycles or if a node depends on an undefined node or variable.</li> </ul> <p>Example Usage:</p> <pre><code>flow.build_graph()\n</code></pre>"},{"location":"api_reference/api_reference_/#run","title":"<code>run</code>","text":"<pre><code>def run(self):\n</code></pre> <p>Description:</p> <p>Executes the nodes in the order determined by the topological sort of the execution graph. It renders the parameters for each node using the outputs of previously executed nodes and handles iterative execution for nodes processing lists.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If there are cycles in the graph or if an error occurs during node execution.</li> </ul> <p>Example Usage:</p> <pre><code>flow.run()\n</code></pre> <p>After execution, the outputs from each node are stored in the <code>outputs</code> attribute of the <code>GenFlow</code> instance.</p>"},{"location":"api_reference/api_reference_/#node","title":"Node","text":"<p>Module: <code>genflow.py</code></p> <p>The <code>Node</code> class represents an individual operation or step within the workflow. It encapsulates the logic required to execute that step, including parameter rendering and function execution.</p>"},{"location":"api_reference/api_reference_/#class-definition_1","title":"Class Definition","text":"<pre><code>class Node:\n    def __init__(self, node_data):\n        # Initialization code\n\n    def set_flow(self, flow):\n        # Sets reference to the GenFlow instance\n\n    def get_dependencies(self, node_names):\n        # Retrieves the dependencies of the node\n\n    def render_params(self, outputs, env):\n        # Renders the parameters using previous outputs\n\n    def execute(self, params):\n        # Executes the node based on its type and parameters\n</code></pre>"},{"location":"api_reference/api_reference_/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(self, node_data):\n</code></pre> <p>Parameters:</p> <ul> <li><code>node_data</code> (dict): Dictionary containing the node's configuration from the YAML file.</li> </ul> <p>Description:</p> <p>Initializes the <code>Node</code> instance with the given configuration. It extracts essential information such as the node's name, type, outputs, and parameters.</p>"},{"location":"api_reference/api_reference_/#methods_1","title":"Methods","text":""},{"location":"api_reference/api_reference_/#set_flow","title":"<code>set_flow</code>","text":"<pre><code>def set_flow(self, flow):\n</code></pre> <p>Parameters:</p> <ul> <li><code>flow</code> (GenFlow): Reference to the <code>GenFlow</code> instance managing the workflow execution.</li> </ul> <p>Description:</p> <p>Sets the reference to the <code>GenFlow</code> instance, allowing the node to access shared resources and configurations during execution.</p>"},{"location":"api_reference/api_reference_/#get_dependencies","title":"<code>get_dependencies</code>","text":"<pre><code>def get_dependencies(self, node_names):\n</code></pre> <p>Parameters:</p> <ul> <li><code>node_names</code> (Iterable[str]): Iterable of all node names in the workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>dependencies</code> (Set[str]): Set of node names that the current node depends on.</li> </ul> <p>Description:</p> <p>Analyzes the node's parameters to determine which other nodes it depends on. This is used to build the execution graph and ensure correct execution order.</p> <p>Example Usage:</p> <pre><code>dependencies = node.get_dependencies(flow.nodes.keys())\n</code></pre>"},{"location":"api_reference/api_reference_/#render_params","title":"<code>render_params</code>","text":"<pre><code>def render_params(self, outputs, env):\n</code></pre> <p>Parameters:</p> <ul> <li><code>outputs</code> (dict): Outputs from previously executed nodes.</li> <li><code>env</code> (jinja2.Environment): Jinja2 environment used for templating.</li> </ul> <p>Returns:</p> <ul> <li><code>rendered_params</code> (dict or list of dicts): Parameters with values rendered using the outputs of previous nodes.</li> </ul> <p>Description:</p> <p>Renders the node's parameters by substituting placeholders with actual values from previous outputs. Supports handling of indexed parameters and lists for iterative processing.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If a referenced variable is not found or is not iterable when expected.</li> </ul>"},{"location":"api_reference/api_reference_/#execute","title":"<code>execute</code>","text":"<pre><code>def execute(self, params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters to be used for the node execution.</li> </ul> <p>Returns:</p> <ul> <li><code>outputs</code> (dict): Dictionary of outputs produced by the node execution.</li> </ul> <p>Description:</p> <p>Executes the node based on its type:</p> <ul> <li>For <code>function_call</code> nodes, it executes a Python function.</li> <li>For <code>llm_service</code> nodes, it interacts with an LLM service like OpenAI.</li> </ul> <p>Delegates to specific execution methods depending on the node type.</p> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: If the node type is not supported.</li> <li><code>Exception</code>: If an error occurs during execution.</li> </ul> <p>Example Usage:</p> <pre><code>outputs = node.execute(rendered_params)\n</code></pre>"},{"location":"api_reference/api_reference_/#yamlcompose","title":"YamlCompose","text":"<p>Module: <code>yaml_utils.py</code></p> <p>The <code>YamlCompose</code> class is responsible for composing multiple YAML workflow files into a single unified workflow. It resolves references to nested workflows (<code>yml_flow</code> nodes) and adjusts node names and parameters to ensure uniqueness and consistency.</p>"},{"location":"api_reference/api_reference_/#class-definition_2","title":"Class Definition","text":"<pre><code>class YamlCompose:\n    def __init__(self, yaml_file, functions_filepath, structured_output_schema_filepath):\n        # Initialization code\n\n    def compose(self, save_combined_yaml=False, output_file='combined.yaml'):\n        # Starts the composition process and returns the combined YAML data\n</code></pre>"},{"location":"api_reference/api_reference_/#constructor_2","title":"Constructor","text":"<pre><code>def __init__(self, yaml_file, functions_filepath, structured_output_schema_filepath):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the root YAML file to be composed.</li> <li><code>functions_filepath</code> (str): Path to the Python file containing custom functions.</li> <li><code>structured_output_schema_filepath</code> (str): Path to the Python file containing structured output schemas.</li> </ul> <p>Description:</p> <p>Initializes the <code>YamlCompose</code> instance and prepares for the composition process by validating the provided file paths.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the provided file paths do not exist.</li> <li><code>ValueError</code>: If the provided file paths are not <code>.py</code> files.</li> </ul>"},{"location":"api_reference/api_reference_/#methods_2","title":"Methods","text":""},{"location":"api_reference/api_reference_/#compose","title":"<code>compose</code>","text":"<pre><code>def compose(self, save_combined_yaml=False, output_file='combined.yaml'):\n</code></pre> <p>Parameters:</p> <ul> <li><code>save_combined_yaml</code> (bool, optional): If <code>True</code>, saves the combined YAML data to a file.</li> <li><code>output_file</code> (str, optional): Filename to save the combined YAML data.</li> </ul> <p>Returns:</p> <ul> <li><code>combined_data</code> (dict): The combined YAML data after composition.</li> </ul> <p>Description:</p> <p>Starts the composition process by recursively processing the root YAML file and any nested sub-flows. Adjusts node names and parameter references to ensure uniqueness across the combined workflow.</p> <p>Raises:</p> <ul> <li><code>Exception</code>: If validation fails during composition.</li> </ul> <p>Example Usage:</p> <pre><code>composer = YamlCompose('main_workflow.yaml', 'functions.py', 'schemas.py')\ncombined_yaml_data = composer.compose(save_combined_yaml=True, output_file='combined.yaml')\n</code></pre> <p>After composition, the combined YAML file can be executed as a single workflow.</p>"},{"location":"api_reference/api_reference_/#utility-functions","title":"Utility Functions","text":"<p>This section documents the utility functions used within GenSphere, primarily for internal processing and validation.</p>"},{"location":"api_reference/api_reference_/#get_function_schema","title":"get_function_schema","text":"<p>Module: <code>genflow.py</code></p> <pre><code>def get_function_schema(func):\n</code></pre> <p>Parameters:</p> <ul> <li><code>func</code> (function): The Python function object to generate a schema for.</li> </ul> <p>Returns:</p> <ul> <li><code>function_def</code> (dict): A dictionary representing the function definition, including name, description, and parameters.</li> </ul> <p>Description:</p> <p>Generates a schema for a given function by inspecting its signature and docstring. This schema is used for OpenAI's function calling feature in LLM service nodes. It ensures that the function parameters are properly typed and documented.</p> <p>Raises:</p> <ul> <li><code>ValueError</code>: If a parameter lacks a type annotation or if the function lacks a docstring.</li> </ul> <p>Example Usage:</p> <p>Used internally when preparing function definitions for OpenAI's function calling.</p>"},{"location":"api_reference/api_reference_/#validate_yaml","title":"validate_yaml","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def validate_yaml(\n    yaml_file,\n    functions_filepath=None,\n    structured_output_schema_filepath=None,\n    parent_node_names=None,\n    visited_files=None,\n    parent_params=None,\n    parent_node_outputs=None\n):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file being validated.</li> <li><code>functions_filepath</code> (str, optional): Path to the functions file.</li> <li><code>structured_output_schema_filepath</code> (str, optional): Path to the schemas file.</li> <li><code>parent_node_names</code> (Set[str], optional): Set of node names from the parent flow.</li> <li><code>visited_files</code> (Set[str], optional): Set of visited YAML files to prevent circular references.</li> <li><code>parent_params</code> (Set[str], optional): Set of parameter names passed from the parent flow.</li> <li><code>parent_node_outputs</code> (Dict[str, List[str]], optional): Dictionary of node outputs from parent flows.</li> </ul> <p>Returns:</p> <ul> <li><code>validated</code> (bool): <code>True</code> if validation passes, <code>False</code> otherwise.</li> <li><code>error_msgs</code> (List[str]): List of error messages encountered during validation.</li> <li><code>node_outputs</code> (Dict[str, List[str]]): Dictionary of node outputs in the current flow.</li> </ul> <p>Description:</p> <p>Validates a YAML workflow file and any associated sub-flows for consistency and correctness. Checks for issues such as:</p> <ul> <li>Missing required fields (<code>name</code>, <code>type</code>).</li> <li>Duplicate node names.</li> <li>Undefined or duplicate outputs.</li> <li>References to undefined nodes or outputs.</li> <li>Cycles in the execution graph.</li> <li>Validity of functions and schemas.</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If referenced files do not exist.</li> <li><code>ValueError</code>: If the YAML structure is invalid.</li> </ul> <p>Example Usage:</p> <p>Used internally before executing or composing workflows to ensure they are valid.</p>"},{"location":"api_reference/api_reference_/#collect_referenced_nodes_and_outputs","title":"collect_referenced_nodes_and_outputs","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_referenced_nodes_and_outputs(params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters dictionary from a node.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes_outputs</code> (Set[Tuple[str, str]]): A set of tuples containing referenced node names and outputs.</li> </ul> <p>Description:</p> <p>Analyzes the parameters of a node to identify all referenced nodes and their outputs, which is essential for validating dependencies and ensuring that all references are valid.</p>"},{"location":"api_reference/api_reference_/#collect_used_params","title":"collect_used_params","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_used_params(yaml_data):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_data</code> (dict): The YAML data of a workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>used_params</code> (Set[str]): A set of parameter names used within the workflow.</li> </ul> <p>Description:</p> <p>Collects all parameter names that are used in the workflow, particularly in the context of nested workflows (<code>yml_flow</code> nodes). This helps in validating that all required parameters are provided.</p>"},{"location":"api_reference/api_reference_/#collect_referenced_params","title":"collect_referenced_params","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_referenced_params(params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters dictionary from a node.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_params</code> (Set[str]): A set of parameter names referenced in the parameters.</li> </ul> <p>Description:</p> <p>Identifies all parameter names that are referenced within the node's parameters, usually in templated strings. This is used to ensure that all referenced parameters are defined.</p>"},{"location":"api_reference/api_reference_/#collect_referenced_nodes","title":"collect_referenced_nodes","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def collect_referenced_nodes(params):\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict): Parameters dictionary from a node.</li> </ul> <p>Returns:</p> <ul> <li><code>referenced_nodes</code> (Set[str]): A set of node names referenced in the parameters.</li> </ul> <p>Description:</p> <p>Identifies all node names that are referenced within the node's parameters. This is crucial for building the execution graph and determining the correct execution order.</p>"},{"location":"api_reference/api_reference_/#load_yaml_file","title":"load_yaml_file","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def load_yaml_file(yaml_file):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_file</code> (str): Path to the YAML file to load.</li> </ul> <p>Returns:</p> <ul> <li><code>data</code> (dict): The loaded YAML data.</li> </ul> <p>Description:</p> <p>Loads the YAML data from a file and handles parsing errors. Ensures that the file exists and contains valid YAML.</p> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the YAML file does not exist.</li> <li><code>ValueError</code>: If there is an error parsing the YAML file.</li> </ul>"},{"location":"api_reference/api_reference_/#has_yml_flow_nodes","title":"has_yml_flow_nodes","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def has_yml_flow_nodes(yaml_data):\n</code></pre> <p>Parameters:</p> <ul> <li><code>yaml_data</code> (dict): The YAML data of a workflow.</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: <code>True</code> if the workflow contains any <code>yml_flow</code> nodes, <code>False</code> otherwise.</li> </ul> <p>Description:</p> <p>Checks whether the given YAML data contains any nested workflows (<code>yml_flow</code> nodes). This helps determine if composition is necessary before execution.</p>"},{"location":"api_reference/api_reference_/#get_base_output_name","title":"get_base_output_name","text":"<p>Module: <code>yaml_utils.py</code></p> <pre><code>def get_base_output_name(output_reference):\n</code></pre> <p>Parameters:</p> <ul> <li><code>output_reference</code> (str): A string representing an output reference (e.g., <code>'countries_list[i]'</code>).</li> </ul> <p>Returns:</p> <ul> <li><code>base_output_name</code> (str): The base output name extracted from the reference.</li> </ul> <p>Description:</p> <p>Extracts the base output name from a complex output reference that may include indexing or attribute access. Used during validation to identify the actual outputs being referenced.</p>"},{"location":"api_reference/api_reference_/#additional-information","title":"Additional Information","text":"<p>These utility functions are primarily used internally by GenSphere to process and validate workflows. Understanding them can be helpful for advanced users who wish to extend or debug the framework.</p> <p>Note: When developing custom functions or schemas for use in GenSphere workflows, ensure that:</p> <ul> <li>Functions have proper docstrings and type annotations.</li> <li>Schemas are defined using Pydantic models.</li> <li>Functions and schemas are placed in the files specified when initializing <code>GenFlow</code> or <code>YamlCompose</code>.</li> </ul>"},{"location":"api_reference/api_reference_/#conclusion","title":"Conclusion","text":"<p>This API reference provides detailed insights into the core classes and utility functions of GenSphere's workflow execution engine. With this knowledge, developers can better understand how GenSphere processes workflows, handles dependencies, and executes tasks.</p> <p>For more examples and usage instructions, refer to the User Guide and Tutorials.</p> <p>If you have any questions or need further assistance, reach out on our GitHub Issues page.</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>Welcome to the GenSphere documentation! This section will guide you through the installation process of GenSphere and its necessary dependencies.</p>"},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing GenSphere, ensure that your system meets the following prerequisites:</p>"},{"location":"getting_started/installation/#1-python","title":"1. Python","text":"<ul> <li>Version: Python 3.10 or higher is required.</li> <li>Installation:</li> <li>Windows: Download and install Python from the official website.</li> <li>macOS: Python is pre-installed on macOS. However, it's recommended to install the latest version using Homebrew:     <code>bash     brew install python</code></li> <li>Linux: Use your distribution's package manager. For example, on Ubuntu:     <code>bash     sudo apt update     sudo apt install python3 python3-pip</code></li> </ul>"},{"location":"getting_started/installation/#2-api-keys","title":"2. API Keys","text":"<p>GenSphere integrates with various external services that require API keys. Ensure you have the following:</p> <ul> <li>OpenAI API Key: Sign up and obtain an API key from OpenAI.</li> <li>Composio API Key (optional): If you plan to use Composio tools, obtain an API key from Composio. Follow the Composio documentation to understand how to add services to your account.</li> </ul>"},{"location":"getting_started/installation/#installation-steps","title":"Installation Steps","text":"<p>Follow these steps to install GenSphere and its dependencies:</p>"},{"location":"getting_started/installation/#1-create-a-virtual-environment-recommended","title":"1. Create a Virtual Environment (Recommended)","text":"<p>It's recommended to create a virtual environment to manage your project's dependencies. Open a terminal command window and type:</p> <pre><code># Create a virtual environment named 'venv'\npython3 -m venv venv\n\n# Activate the virtual environment\n# On Windows\nvenv\\Scripts\\activate\n\n# On macOS and Linux\nsource venv/bin/activate\n</code></pre>"},{"location":"getting_started/installation/#2-upgrade-pip","title":"2. Upgrade pip","text":"<p>Ensure that you have the latest version of <code>pip</code>.</p> <pre><code>pip install --upgrade pip\n</code></pre>"},{"location":"getting_started/installation/#3-install-gensphere","title":"3. Install GenSphere","text":"<p>Install GenSphere using <code>pip</code>.</p> <pre><code>pip install gensphere\n</code></pre> <p>Note: If you encounter permission issues, you may need to use <code>pip</code> with <code>--user</code> or consider using a virtual environment as shown above.</p>"},{"location":"getting_started/installation/#4-verify-the-installation","title":"4. Verify the Installation","text":"<p>To verify that GenSphere has been installed correctly, you can perform a simple test by importing GenSphere in Python.</p> <pre><code>python -c \"import gensphere; print('GenSphere installed successfully!')\"\n</code></pre> <p>If the installation was successful, you should see the following output:</p> <pre><code>GenSphere installed successfully!\n</code></pre>"},{"location":"getting_started/installation/#additional-dependencies","title":"Additional Dependencies","text":"<p>Depending on your project's requirements, you might need to install additional dependencies. </p>"},{"location":"getting_started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following steps:</p> <ol> <li>Check Python Version: Ensure you're using Python 3.10 or higher.</li> <li>Upgrade pip: Make sure <code>pip</code> is up to date.</li> <li>Virtual Environment: Use a virtual environment to avoid dependency conflicts.</li> <li>Contact Support: If you're still having trouble,join our Discord server.</li> </ol> <p>Proceed to the Quickstart Guide to begin creating and running your first GenSphere workflow!</p>"},{"location":"getting_started/quickstart/","title":"Quickstart Guide","text":"<p>Welcome to the GenSphere Quickstart Guide! This guide will walk you through creating and executing your first GenSphere workflow. By the end of this guide, you will have a functional workflow that interacts with the GenSphere platform.</p>"},{"location":"getting_started/quickstart/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Setting Up Environment Variables</li> <li>Importing GenSphere Modules</li> <li>Running Your First Flow<ul> <li>Pulling a Project from the Platform</li> <li>Executing the Flow</li> <li>Accessing Outputs</li> </ul> </li> <li>Next Steps</li> </ol>"},{"location":"getting_started/quickstart/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>Before you can run GenSphere workflows, you need to set up the necessary environment variables, including your API keys.</p>"},{"location":"getting_started/quickstart/#1-obtain-api-keys","title":"1. Obtain API Keys","text":"<p>Ensure you have the following API keys:</p> <ul> <li>OpenAI API Key: Obtain here.</li> <li>Composio API Key (optional): Obtain here.</li> </ul>"},{"location":"getting_started/quickstart/#2-define-environment-variables","title":"2. Define Environment Variables","text":"<p>Set your environment variables in your terminal or include them in a <code>.env</code> file. Using a <code>.env</code> file is recommended for convenience and security.</p>"},{"location":"getting_started/quickstart/#using-a-env-file","title":"Using a <code>.env</code> File","text":"<p>Create a file named <code>.env</code> in your project directory and add the following lines:</p> <pre><code>OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"\n</code></pre> <p>Substitute \"YOUR_API_KEY\" by your actual api key.</p>"},{"location":"getting_started/quickstart/#loading-environment-variables","title":"Loading Environment Variables","text":"<p>Ensure that your Python environment loads the <code>.env</code> file. GenSphere uses the <code>python-dotenv</code> package to load environment variables from a <code>.env</code> file automatically.</p> <p>If you haven't installed <code>python-dotenv</code>, install it using <code>pip</code>:</p> <pre><code>pip install python-dotenv\n</code></pre> <p>Alternatively, you can manually set environment variables in your terminal session:</p> <pre><code># On Windows\nset OPENAI_API_KEY=YOUR_OPENAI_API_KEY\n\n# On macOS and Linux\nexport OPENAI_API_KEY=YOUR_OPENAI_API_KEY\n</code></pre>"},{"location":"getting_started/quickstart/#importing-gensphere-modules","title":"Importing GenSphere Modules","text":"<p>Begin by importing the necessary GenSphere modules in your Python script or Jupyter Notebook.</p> <pre><code>import logging\nimport traceback\n\n\n# Set up logging configuration before importing other modules\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"app.log\", mode='w'),\n        logging.StreamHandler()\n    ]\n)\n\nfrom gensphere import genflow, yaml_utils\nfrom gensphere.genflow import GenFlow\nfrom gensphere.yaml_utils import YamlCompose\nfrom gensphere.visualizer import Visualizer\nfrom gensphere.hub import Hub\n</code></pre>"},{"location":"getting_started/quickstart/#explanation-of-imports","title":"Explanation of Imports","text":"<ul> <li>logging &amp; traceback: For logging and debugging.</li> <li>gensphere modules: Core classes and utilities for defining and running workflows.</li> </ul>"},{"location":"getting_started/quickstart/#running-your-first-flow","title":"Running Your First Flow","text":"<p>This section guides you through pulling a pre-built workflow from the GenSphere platform, executing it, and accessing its outputs.</p>"},{"location":"getting_started/quickstart/#pulling-a-project-from-the-platform","title":"Pulling a Project from the Platform","text":"<p>GenSphere allows you to pull pre-built workflows from its open platform using a unique <code>push_id</code>. This enables you to quickly start with existing workflows.</p> <pre><code># Initialize the Hub\nhub = Hub()\n\n# Define the push_id and filenames to save the pulled files\npush_id = '2c03079c-0e33-489e-bbbe-777da744d56f'\nyaml_filename = 'simple_examples.yaml' #this will be the path of your YAML file after downloading it\nfunctions_filename = 'simple_examples_functions.py' #this will be the path of your functions file after downloading it\n\n# Pull the project from the platform\nhub.pull(\n    push_id=push_id,\n    yaml_filename=yaml_filename,\n    functions_filename=functions_filename,\n    save_to_disk=True\n)\n</code></pre> <p>Explanation:</p> <ul> <li>Hub: The <code>Hub</code> class manages interactions with the GenSphere platform.</li> <li>push_id: Unique identifier for the project you want to pull.</li> <li>yaml_filename &amp; functions_filename: Names of the files to save locally.</li> </ul> <p>Result:</p> <p>This command downloads the specified YAML workflow and associated Python functions, saving them to your current working directory.</p>"},{"location":"getting_started/quickstart/#executing-the-flow","title":"Executing the Flow","text":"<p>Once you've pulled the workflow files, you can execute the workflow using the <code>GenFlow</code> class.</p> <pre><code># Initialize GenFlow with the pulled YAML and functions files\nflow = GenFlow('simple_examples.yaml', 'simple_examples_functions.py')\n\n# Parse the YAML file to construct the execution graph\nflow.parse_yaml()\n\n# Run the workflow\nflow.run()\n</code></pre> <p>Explanation:</p> <ul> <li>GenFlow: Core class responsible for parsing, building, and executing workflows.</li> <li>parse_yaml(): Parses the YAML file and constructs the execution graph.</li> <li>run(): Executes the workflow based on the constructed graph.</li> </ul>"},{"location":"getting_started/quickstart/#accessing-outputs","title":"Accessing Outputs","text":"<p>After running the workflow, you can access the outputs generated by each node.</p> <pre><code># Access the outputs of the workflow\noutputs = flow.outputs\n\n# Print the outputs\nfor node_name, output in outputs.items():\n    print(f\"Outputs from node '{node_name}': {output}\")\n</code></pre> <p>Explanation:</p> <ul> <li>flow.outputs: A dictionary containing the outputs from each node in the workflow.</li> <li>Iterating through outputs: Print or utilize the outputs as needed.</li> </ul> <p>Example Output:</p> <pre><code>Outputs from node 'get_current_date': {'current_date': '2024-04-27'}\nOutputs from node 'get_timewindow': {'time_window': 'last month'}\nOutputs from node 'product_hunt_scrape': {'product_hunt_scrape_results': ...}\n</code></pre>"},{"location":"getting_started/quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've successfully installed GenSphere, pulled a workflow from the platform, executed it, and accessed its outputs. Here are some suggested next steps to further enhance your understanding and usage of GenSphere:</p> <ol> <li>Explore the Tutorials: Dive into the Quickstart Tutorial for more detailed guidance.</li> <li>Define Your Own Workflows: Start creating your own workflows by defining YAML files, custom functions, and schemas.</li> <li>Integrate with Tools: Leverage integrations with LangChain and Composio to enhance your workflows.</li> <li>Visualize Your Workflows: Use the <code>Visualizer</code> class to get a graphical representation of your workflows.</li> <li>Contribute to GenSphere: Share your workflows and contribute to the GenSphere community by pushing your projects to the platform.</li> </ol> <p>Continue to the Tutorials section to deepen your understanding of GenSphere's capabilities! ```</p>"},{"location":"tutorials/tutorial/","title":"GenSphere Tutorial","text":"<p>Welcome to the GenSphere tutorial! In this guide, we'll walk you through the main functionalities of GenSphere, an AI agent development framework that simplifies the creation and execution of complex workflows involving functions and language models.</p> <p>By completing this tutorial, you will learn how to:</p> <ol> <li>Define workflows using YAML files.</li> <li>Use pre-built components from the GenSphere platform.</li> <li>Nest workflows to create complex pipelines.</li> <li>Utilize custom functions and schemas, and integrate with LangChain and Composio tools.</li> <li>Visualize workflows for better understanding.</li> <li>Push and pull workflows to and from the GenSphere platform.</li> </ol> <p>You can also run this example directly on Google Colab here. Let's get started!</p>"},{"location":"tutorials/tutorial/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Importing GenSphere</li> <li>Setting Up Environment Variables</li> <li>Defining Your Workflow with YAML</li> <li>4.1 Pulling a Base YAML File</li> <li>4.2 Visualizing Your Project</li> <li>4.3 Understanding the YAML Syntax<ul> <li>Function Call Nodes</li> <li>LLM Service Nodes</li> <li>YML Flow Nodes</li> <li>Working with Lists</li> </ul> </li> <li>Combining Workflows</li> <li>Running Your Project</li> <li>Pushing to the Platform</li> <li>Checking Project Popularity</li> <li>Conclusion</li> </ol>"},{"location":"tutorials/tutorial/#1-installation","title":"1. Installation","text":"<p>First, ensure you have Python 3.10 or higher installed on your system. Then, install GenSphere and other required libraries using <code>pip</code>:</p> <pre><code>pip install gensphere\n</code></pre>"},{"location":"tutorials/tutorial/#2-importing-gensphere","title":"2. Importing GenSphere","text":"<p>In your Python script or Jupyter notebook, import the necessary modules:</p> <pre><code>import logging\nimport os\nfrom gensphere.genflow import GenFlow\nfrom gensphere.yaml_utils import YamlCompose\nfrom gensphere.visualizer import Visualizer\nfrom gensphere.hub import Hub\n</code></pre> <p>Set up logging to monitor the execution:</p> <pre><code>logging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"app.log\", mode='w'),\n        logging.StreamHandler()\n    ]\n)\n</code></pre>"},{"location":"tutorials/tutorial/#3-setting-up-environment-variables","title":"3. Setting Up Environment Variables","text":"<p>If you haven't defined your enviroment variables yet, you can do so now. Replace the placeholders with your actual API keys. You'll need API keys for OpenAI, Composio, and FireCrawl.</p> <pre><code>os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY'\nos.environ['COMPOSIO_API_KEY'] = 'YOUR_COMPOSIO_API_KEY'  # Visit composio.dev to get one\nos.environ['FIRECRAWL_API_KEY'] = 'YOUR_FIRECRAWL_API_KEY'  # Visit firecrawl.dev to get one\n</code></pre>"},{"location":"tutorials/tutorial/#4-defining-your-workflow-with-yaml","title":"4. Defining Your Workflow with YAML","text":"<p>Our goal is to create a workflow that automatically finds the latest product releases on Product Hunt, explores their revenue and traction, and analyzes a new startup idea based on that information.</p>"},{"location":"tutorials/tutorial/#41-pulling-a-base-yaml-file","title":"4.1 Pulling a Base YAML File","text":"<p>We will use a pre-built workflow from the GenSphere open platform that extracts information from Product Hunt. This workflow will be nested into a larger workflow to achieve our objective.</p>"},{"location":"tutorials/tutorial/#pulling-from-the-platform","title":"Pulling from the Platform","text":"<p>Use the <code>Hub</code> class to pull the base YAML file, along with its associated functions and schema files:</p> <pre><code># Define paths to save the files\npath_to_save_yaml_file = 'product_hunt_analyzer.yaml'\npath_to_save_functions_file = 'gensphere_functions.py'\npath_to_save_schema_file = 'structured_output_schema.py'\n\n# Initialize the Hub\nhub = Hub()\n\n# Pull the files using the push_id\nhub.pull(\n    push_id='de8afbeb-06cb-4f8f-8ead-64d9e6ef5326',\n    yaml_filename=path_to_save_yaml_file,\n    functions_filename=path_to_save_functions_file,\n    schema_filename=path_to_save_schema_file,\n    save_to_disk=True\n)\n</code></pre>"},{"location":"tutorials/tutorial/#examining-the-yaml-file","title":"Examining the YAML File","text":"<p>The YAML file <code>product_hunt_analyzer.yaml</code> has been saved locally. Here's the content:</p> <pre><code># product_hunt_analyzer.yaml\n\nnodes:\n  - name: get_current_date\n    type: function_call\n    function: get_current_date_function\n    outputs:\n      - current_date\n\n  - name: get_timewindow\n    type: function_call\n    function: get_timewindow_function\n    outputs:\n      - time_window\n\n  - name: product_hunt_scrape\n    type: llm_service\n    service: openai\n    model: \"gpt-4o-2024-08-06\"\n    tools:\n      - COMPOSIO.FIRECRAWL_SCRAPE\n    params:\n      prompt: |\n        You should visit Product Hunt at https://www.producthunt.com/leaderboard/monthly/yyyy/mm\n        Today is {{ get_current_date.current_date }}\n        Substitute yyyy and mm with the year and month you want to search.\n        The search time window should be {{ get_timewindow.time_window }}.\n        Extract raw content from the HTML pages, which contain information about new product launches, companies, number of upvotes, etc.\n        Scroll the page until the end and wait a few milliseconds before scraping.\n\n    outputs:\n      - product_hunt_scrape_results\n\n  - name: extract_info_from_search\n    type: llm_service\n    service: openai\n    model: \"gpt-4o-2024-08-06\"\n    structured_output_schema: StartupInformationList\n    params:\n      prompt: |\n        You are given reports from a search on Product Hunt containing products featured last month:\n        {{ product_hunt_scrape.product_hunt_scrape_results }}.\n        Extract accurate information about these new product launches.\n        Structure the information with the following dimensions: product name, company name, company URL, number of upvotes, business model, and brief description.\n\n    outputs:\n      - structured_search_info\n\n  - name: postprocess_search_results\n    type: function_call\n    function: postprocess_search_results_function\n    params:\n      info: '{{ extract_info_from_search.structured_search_info }}'\n    outputs:\n      - postprocessed_search_results\n\n  - name: find_extra_info\n    type: llm_service\n    service: openai\n    model: \"gpt-4o-2024-08-06\"\n    tools:\n      - COMPOSIO.TAVILY_TAVILY_SEARCH\n    params:\n      prompt: |\n        Conduct a comprehensive web search about the following entry from Product Hunt:\n        {{ postprocess_search_results.postprocessed_search_results[i] }}.\n        Find relevant news about the company, especially related to revenue, valuation, traction, acquisition, number of users, etc.\n\n    outputs:\n      - startup_extra_info\n</code></pre>"},{"location":"tutorials/tutorial/#42-visualizing-your-project","title":"4.2 Visualizing Your Project","text":"<p>To better understand the workflow, use the <code>Visualizer</code> class to visualize the project:</p> <pre><code>viz = Visualizer(\n    yaml_file='product_hunt_analyzer.yaml',\n    functions_file='gensphere_functions.py',\n    schema_file='structured_output_schema.py',\n    address='127.0.0.1',\n    port=8050\n)\nviz.start_visualization()\n</code></pre> <p>Note: Running the visualization inside environments like Google Colab might be cumbersome. It's recommended to run it locally and access it through your browser.</p>"},{"location":"tutorials/tutorial/#43-understanding-the-yaml-syntax","title":"4.3 Understanding the YAML Syntax","text":"<p>GenSphere uses a YAML-based syntax to define workflows. There are three types of nodes:</p> <ol> <li>Function Call Nodes (<code>function_call</code>)</li> <li>LLM Service Nodes (<code>llm_service</code>)</li> <li>YML Flow Nodes (<code>yml_flow</code>)</li> </ol>"},{"location":"tutorials/tutorial/#function-call-nodes","title":"Function Call Nodes","text":"<p>These nodes trigger the execution of Python functions defined in a separate <code>.py</code> file. They have <code>params</code> and <code>outputs</code> fields.</p> <p>Example:</p> <pre><code>- name: get_current_date\n  type: function_call\n  function: get_current_date_function\n  outputs:\n    - current_date\n</code></pre> <p>Function Definition (<code>gensphere_functions.py</code>):</p> <pre><code># gensphere_functions.py\n\nfrom datetime import datetime\n\ndef get_current_date_function():\n    \"\"\"\n    Returns the current date as a string.\n\n    Returns:\n        dict: A dictionary with 'current_date' as key and current date as value.\n    \"\"\"\n    return {'current_date': datetime.today().strftime('%Y-%m-%d')}\n</code></pre> <p>Key Points:</p> <ul> <li>Function Outputs: The function must return a dictionary whose keys match the <code>outputs</code> defined in the YAML file.</li> <li>Referencing Outputs: Use the syntax <code>{{ node_name.output_name }}</code> to reference outputs from other nodes in the <code>params</code> field.</li> </ul>"},{"location":"tutorials/tutorial/#llm-service-nodes","title":"LLM Service Nodes","text":"<p>These nodes execute calls to language model APIs. Currently, GenSphere supports OpenAI's API, including structured outputs and function calling.</p> <p>Example:</p> <pre><code>- name: product_hunt_scrape\n  type: llm_service\n  service: openai\n  model: \"gpt-4o-2024-08-06\"\n  tools:\n    - COMPOSIO.FIRECRAWL_SCRAPE\n  params:\n    prompt: |\n      You should visit Product Hunt at https://www.producthunt.com/leaderboard/monthly/yyyy/mm\n      Today is {{ get_current_date.current_date }}\n      Substitute yyyy and mm with the year and month you want to search.\n      The search time window should be {{ get_timewindow.time_window }}.\n      Extract raw content from the HTML pages, which contain information about new product launches, companies, number of upvotes, etc.\n      Scroll the page until the end and wait a few milliseconds before scraping.\n\n  outputs:\n    - product_hunt_scrape_results\n</code></pre> <p>Key Points:</p> <ul> <li>Tools Field: Can refer to functions in your <code>.py</code> file, Composio tools (<code>COMPOSIO.tool_name</code>), or LangChain tools (<code>LANGCHAIN.tool_name</code>).</li> <li>Structured Outputs: Use the <code>structured_output_schema</code> field to specify a Pydantic schema for the expected output.</li> </ul> <p>Structured Output Example:</p> <pre><code>- name: extract_info_from_search\n  type: llm_service\n  service: openai\n  model: \"gpt-4o-2024-08-06\"\n  structured_output_schema: StartupInformationList\n  params:\n    prompt: |\n      You are given reports from a search on Product Hunt containing products featured last month:\n      {{ product_hunt_scrape.product_hunt_scrape_results }}.\n      Extract accurate information about these new product launches.\n      Structure the information with the following dimensions: product name, company name, company URL, number of upvotes, business model, and brief description.\n\n  outputs:\n    - structured_search_info\n</code></pre> <p>Schema Definition (<code>structured_output_schema.py</code>):</p> <pre><code># structured_output_schema.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass StartupInformation(BaseModel):\n    product_name: str = Field(..., description=\"The name of the product\")\n    company_name: str = Field(..., description=\"The name of the company offering the product\")\n    url: str = Field(..., description=\"URL associated with the product\")\n    number_upvotes: int = Field(..., description=\"Number of upvotes the product has received\")\n    business_model: str = Field(..., description=\"Brief description of the business model\")\n    brief_description: str = Field(..., description=\"Brief description of the product\")\n\nclass StartupInformationList(BaseModel):\n    information_list: List[StartupInformation]\n</code></pre> <p>Post-Processing Node:</p> <p>After obtaining structured output, we want post-process it. The output is an instance of the class <code>StartupInformationList</code>, which is a list of <code>StartupInformation</code> instances, as defined in the pydantic mode. We want to extract this as a list, so we applu the <code>postprocess_search_results_function</code>.</p> <pre><code>- name: postprocess_search_results\n  type: function_call\n  function: postprocess_search_results_function\n  params:\n    info: '{{ extract_info_from_search.structured_search_info }}'\n  outputs:\n    - postprocessed_search_results\n</code></pre> <p>Function Definition (<code>gensphere_functions.py</code>):</p> <pre><code>def postprocess_search_results_function(info):\n    \"\"\"\n    Processes the structured search information.\n\n    Args:\n        info (StartupInformationList): The structured search info.\n\n    Returns:\n        dict: A dictionary with 'postprocessed_search_results' as key.\n    \"\"\"\n    result = info.model_dump().get('information_list')\n    return {'postprocessed_search_results': result}\n</code></pre>"},{"location":"tutorials/tutorial/#yml-flow-nodes","title":"YML Flow Nodes","text":"<p>These nodes represent entire YAML files themselves, allowing you to nest workflows.</p> <p>Example:</p> <pre><code>- name: product_hunt_analyzer\n  type: yml_flow\n  yml_file: product_hunt_analyzer.yaml\n  outputs:\n    - postprocessed_search_results\n    - startup_extra_info\n</code></pre> <p>Key Points:</p> <ul> <li>Nested Workflows: GenSphere will handle dependencies and compose a combined YAML file that is ready to run.</li> <li>Parameters: You can pass parameters to the nested workflow using the <code>params</code> field.</li> </ul>"},{"location":"tutorials/tutorial/#working-with-lists","title":"Working with Lists","text":"<p>When the output of a node is a list, you might want to apply the next node to each element individually.</p> <p>Syntax:</p> <ul> <li>Use <code>{{ node_name.output_name[i] }}</code> in the <code>params</code> or <code>prompt</code> field.</li> <li>GenSphere will execute the node for each element and collect the outputs as a list.</li> </ul> <p>Example:</p> <pre><code>- name: find_extra_info\n  type: llm_service\n  service: openai\n  model: \"gpt-4o-2024-08-06\"\n  tools:\n    - COMPOSIO.TAVILY_TAVILY_SEARCH\n  params:\n    prompt: |\n      Conduct a comprehensive web search about the following entry from Product Hunt:\n      {{ postprocess_search_results.postprocessed_search_results[i] }}.\n      Find relevant news about the company, especially related to revenue, valuation, traction, acquisition, number of users, etc.\n\n  outputs:\n    - startup_extra_info\n</code></pre> <p>Key Points:</p> <ul> <li>Iterative Processing: The node will process each item in <code>postprocessed_search_results</code> individually.</li> <li>Collected Outputs: Outputs are collected into a list corresponding to each input item.</li> </ul>"},{"location":"tutorials/tutorial/#5-combining-workflows","title":"5. Combining Workflows","text":"<p>Now, we'll embed the <code>product_hunt_analyzer</code> workflow into a larger workflow to analyze a new startup idea.</p>"},{"location":"tutorials/tutorial/#defining-the-new-workflow","title":"Defining the New Workflow","text":"<p>Create a new YAML file named <code>startup_idea_evaluator.yaml</code>:</p> <pre><code># startup_idea_evaluator.yaml\n\nnodes:\n  - name: read_idea\n    type: function_call\n    function: read_file_as_string\n    params:\n      file_path: \"domains_to_search.txt\"\n    outputs:\n      - domains\n\n  - name: product_hunt_analyzer\n    type: yml_flow\n    yml_file: product_hunt_analyzer.yaml\n    outputs:\n      - postprocessed_search_results\n      - startup_extra_info\n\n  - name: generate_report\n    type: llm_service\n    service: openai\n    model: \"gpt-4o-2024-08-06\"\n    params:\n      prompt: |\n        You are a world-class VC analyst. You are analyzing the following startup idea:\n        {{ read_idea.domains }}\n        Your task is to analyze this idea in the context of recent launches on Product Hunt.\n        Recent launches are:\n        {{ product_hunt_analyzer.postprocessed_search_results }}\n        Additional information about these companies:\n        {{ product_hunt_analyzer.startup_extra_info }}.\n\n        Create a detailed report containing:\n        1. An overview of recent launches on Product Hunt. What are the main ideas being explored?\n        2. A list of companies from Product Hunt that may become direct competitors to the startup idea. Explain your rationale.\n        3. A list of the most promising startups from the Product Hunt launches, based on valuation, revenue, traction, or other relevant metrics.\n        4. A table containing all information found from the Product Hunt launches.\n\n        Answer in markdown format.\n\n    outputs:\n      - report\n</code></pre>"},{"location":"tutorials/tutorial/#composing-the-combined-workflow","title":"Composing the Combined Workflow","text":"<p>Use <code>YamlCompose</code> to create a combined YAML file that resolves all dependencies:</p> <pre><code># Assuming 'startup_idea_evaluator.yaml' is in the current directory\ncomposer = YamlCompose(\n    yaml_file='startup_idea_evaluator.yaml',\n    functions_filepath='gensphere_functions.py',\n    structured_output_schema_filepath='structured_output_schema.py'\n)\ncombined_yaml_data = composer.compose(save_combined_yaml=True, output_file='combined.yaml')\n</code></pre> <p>Note: Ensure all referenced functions and schemas are available in the specified files.</p>"},{"location":"tutorials/tutorial/#visualizing-the-combined-workflow","title":"Visualizing the Combined Workflow","text":"<p>You can visualize the combined workflow to verify that nesting has been handled correctly:</p> <pre><code>viz = Visualizer(\n    yaml_file='combined.yaml',\n    functions_file='gensphere_functions.py',\n    schema_file='structured_output_schema.py',\n    address='127.0.0.1',\n    port=8050\n)\nviz.start_visualization()\n</code></pre>"},{"location":"tutorials/tutorial/#6-running-your-project","title":"6. Running Your Project","text":"<p>Now that you have the combined YAML file and necessary Python files, you can execute the workflow.</p>"},{"location":"tutorials/tutorial/#preparing-input-files","title":"Preparing Input Files","text":"<p>The first node <code>read_idea</code> expects a text file named <code>domains_to_search.txt</code>. Create this file with your startup idea:</p> <pre><code>startup_idea = \"\"\"\nA startup that creates interactive voice agents using generative AI with emphasis on applications like\nlanguage tutoring, entertainment, or mental health. The business model would be B2C.\n\"\"\"\n\nwith open(\"domains_to_search.txt\", \"w\") as text_file:\n    text_file.write(startup_idea)\n</code></pre>"},{"location":"tutorials/tutorial/#executing-the-workflow","title":"Executing the Workflow","text":"<p>Initialize <code>GenFlow</code> and run the workflow:</p> <pre><code>flow = GenFlow(\n    yaml_file='combined.yaml',\n    functions_filepath='gensphere_functions.py',\n    structured_output_schema_filepath='structured_output_schema.py'\n)\nflow.parse_yaml()\nflow.run()\n</code></pre>"},{"location":"tutorials/tutorial/#accessing-the-outputs","title":"Accessing the Outputs","text":"<p>After execution, you can access the results:</p> <pre><code># Access all outputs\noutputs = flow.outputs\n\n# Print the final report\nfinal_report = outputs.get(\"generate_report\").get(\"report\")\n\n# Display the report in Markdown format\nfrom IPython.display import display, Markdown\ndisplay(Markdown(final_report))\n</code></pre>"},{"location":"tutorials/tutorial/#7-pushing-to-the-platform","title":"7. Pushing to the Platform","text":"<p>You can push your project to the GenSphere platform, allowing others to pull and use it.</p> <pre><code>hub = Hub(\n    yaml_file='combined.yaml',\n    functions_file='gensphere_functions.py',\n    schema_file='structured_output_schema.py'\n)\n\nresult = hub.push(push_name='Workflow to analyze startup idea based on recent Product Hunt launches.')\n</code></pre> <p>Retrieve and print the <code>push_id</code>:</p> <pre><code>print(f\"Push ID: {result.get('push_id')}\")\nprint(f\"Uploaded Files: {result.get('uploaded_files')}\")\n</code></pre>"},{"location":"tutorials/tutorial/#8-checking-project-popularity","title":"8. Checking Project Popularity","text":"<p>Check how many times your project has been pulled from the platform:</p> <pre><code># Replace with your actual push_id\npush_id = result.get('push_id')\n\n# Get the total number of pulls for the push_id\ntotal_pulls = hub.count_pulls(push_id=push_id)\nprint(f\"Total pulls for push_id {push_id}: {total_pulls}\")\n</code></pre>"},{"location":"tutorials/tutorial/#9-conclusion","title":"9. Conclusion","text":"<p>Congratulations! You've successfully:</p> <ul> <li>Defined workflows using YAML files.</li> <li>Used pre-built components and nested workflows.</li> <li>Integrated custom functions, schemas, and external tools.</li> <li>Visualized your workflow for better understanding.</li> <li>Executed the workflow and accessed the outputs.</li> <li>Pushed your project to the GenSphere platform.</li> </ul>"},{"location":"tutorials/tutorial/#additional-resources","title":"Additional Resources","text":"<ul> <li>GenSphere GitHub Repository: https://github.com/octopus2023-inc/gensphere</li> <li>Composio Documentation: https://app.composio.dev/sdk_guide</li> <li>LangChain Tools: https://python.langchain.com/api_reference/community/tools.html</li> </ul>"},{"location":"tutorials/tutorial/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues:</p> <ul> <li>API Keys: Ensure all API keys are correctly set as environment variables.</li> <li>Dependencies: Verify that all required packages are installed.</li> <li>File Paths: Double-check the file paths provided to functions and classes.</li> <li>Logs: Check the <code>app.log</code> file for detailed error messages.</li> </ul>"},{"location":"tutorials/tutorial/#feedback","title":"Feedback","text":"<p>Your feedback is valuable. If you have suggestions or find any issues, please open an issue on the GenSphere GitHub repository.</p>"},{"location":"user_guide/execution/","title":"Execution","text":""},{"location":"user_guide/execution/#7-execution","title":"7. Execution","text":"<p>Executing workflows in GenSphere is straightforward using the <code>GenFlow</code> class. This section explains how to execute your workflow and handle outputs.</p>"},{"location":"user_guide/execution/#71-setting-up-the-environment","title":"7.1 Setting Up the Environment","text":"<p>Ensure that:</p> <ul> <li>All required environment variables (e.g., API keys) are set.</li> <li>All necessary files (<code>yaml</code>, <code>functions.py</code>, <code>schemas.py</code>) are available.</li> </ul>"},{"location":"user_guide/execution/#72-executing-the-workflow","title":"7.2 Executing the Workflow","text":"<p>Use the <code>GenFlow</code> class to execute your workflow.</p> <p>Example:</p> <pre><code>from gensphere.genflow import GenFlow\n\nflow = GenFlow(\n    yaml_file='combined.yaml',\n    functions_filepath='functions.py',\n    structured_output_schema_filepath='schemas.py'\n)\nflow.parse_yaml()\nflow.run()\n</code></pre>"},{"location":"user_guide/execution/#73-accessing-outputs","title":"7.3 Accessing Outputs","text":"<p>After execution, outputs from each node are accessible via the <code>outputs</code> attribute.</p> <p>Example:</p> <pre><code># Access all outputs\noutputs = flow.outputs\n\n# Access specific node outputs\ndata_collection_output = outputs.get('data_collection').get('collected_data')\n\n# Print the outputs\nprint(\"Collected Data:\")\nprint(data_collection_output)\n</code></pre>"},{"location":"user_guide/execution/#74-handling-exceptions","title":"7.4 Handling Exceptions","text":"<p>If an error occurs during execution:</p> <ul> <li>Check the log files for detailed error messages.</li> <li>Ensure that all referenced nodes, functions, and schemas exist.</li> <li>Verify that all parameters are correctly specified.</li> </ul>"},{"location":"user_guide/execution/#75-logging","title":"7.5 Logging","text":"<p>GenSphere uses Python's logging module to log execution details.</p> <ul> <li>Logging Levels: Adjust logging levels as needed.   <code>python   import logging   logging.getLogger('gensphere').setLevel(logging.DEBUG)</code></li> <li>Log File: By default, logs may be written to <code>app.log</code> or the console.</li> </ul>"},{"location":"user_guide/execution/#76-execution-flow","title":"7.6 Execution Flow","text":"<p>The execution order of nodes is determined by their dependencies:</p> <ul> <li>Nodes with no dependencies are executed first.</li> <li>Nodes are executed after all their dependencies have completed.</li> </ul>"},{"location":"user_guide/functions_and_schemas/","title":"Functions and Schemas","text":""},{"location":"user_guide/functions_and_schemas/#2-functions-and-schemas","title":"2. Functions and Schemas","text":"<p>In GenSphere, you can define custom functions and schemas to extend the functionality of your workflows.</p>"},{"location":"user_guide/functions_and_schemas/#21-functions","title":"2.1 Functions","text":"<p>Custom functions are Python functions defined in a <code>.py</code> file that you specify when running your workflow. These functions can be used in function_call nodes or as tools in llm_service nodes.</p>"},{"location":"user_guide/functions_and_schemas/#defining-functions","title":"Defining Functions","text":"<ul> <li>Functions must return a dictionary whose keys match the <code>outputs</code> defined in the YAML node.</li> <li>All parameters must be type-annotated.</li> <li>Include docstrings to describe the function's purpose, parameters, and return values.</li> </ul> <p>Example (<code>functions.py</code>):</p> <pre><code>def process_data_function(data: str) -&gt; dict:\n    \"\"\"\n    Processes the input data.\n\n    Args:\n        data (str): The data to process.\n\n    Returns:\n        dict: A dictionary with 'processed_data' as key.\n    \"\"\"\n    # Your processing logic here\n    processed_data = data.upper()  # Example processing\n    return {'processed_data': processed_data}\n</code></pre>"},{"location":"user_guide/functions_and_schemas/#using-functions-in-nodes","title":"Using Functions in Nodes","text":"<p>Example YAML Node:</p> <pre><code>- name: process_data\n  type: function_call\n  function: process_data_function\n  params:\n    data: '{{ read_data.raw_data }}'\n  outputs:\n    - processed_data\n</code></pre>"},{"location":"user_guide/functions_and_schemas/#22-schemas","title":"2.2 Schemas","text":"<p>Schemas are used to define structured outputs for language model responses using Pydantic models.</p>"},{"location":"user_guide/functions_and_schemas/#defining-schemas","title":"Defining Schemas","text":"<p>Create a <code>.py</code> file (e.g., <code>schemas.py</code>) with Pydantic models representing the expected structure of your data.</p> <p>Example (<code>schemas.py</code>):</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass Item(BaseModel):\n    id: int = Field(..., description=\"The unique identifier of the item.\")\n    name: str = Field(..., description=\"The name of the item.\")\n    description: str = Field(..., description=\"A brief description of the item.\")\n\nclass ItemList(BaseModel):\n    items: List[Item]\n</code></pre>"},{"location":"user_guide/functions_and_schemas/#using-schemas-in-nodes","title":"Using Schemas in Nodes","text":"<p>Use the <code>structured_output_schema</code> field in an llm_service node to specify the schema.</p> <p>Example YAML Node:</p> <pre><code>- name: extract_items\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  structured_output_schema: ItemList\n  params:\n    prompt: |\n      Extract item information from the following text:\n      {{ source_text }}\n  outputs:\n    - extracted_items\n</code></pre> <p>Key Points:</p> <ul> <li>The <code>structured_output_schema</code> value should match the class name in your schema file.</li> <li>The language model will attempt to produce output that conforms to the specified schema.</li> </ul>"},{"location":"user_guide/functions_and_schemas/#accessing-structured-outputs","title":"Accessing Structured Outputs","text":"<p>After execution, the output will be an instance of the schema class. You can access the data using standard attribute access.</p> <p>Example:</p> <pre><code>items = flow.outputs['extract_items']['extracted_items']\nfor item in items.items:\n    print(f\"Item ID: {item.id}, Name: {item.name}\")\n</code></pre>"},{"location":"user_guide/integration_with_composio_and_langchain/","title":"Integration with Composio and Langchain tools","text":""},{"location":"user_guide/integration_with_composio_and_langchain/#4-integrations-with-composio-and-langchain","title":"4. Integrations with Composio and Langchain","text":"<p>GenSphere supports integration with external tools and platforms to enhance your workflows.</p>"},{"location":"user_guide/integration_with_composio_and_langchain/#41-composio-tools","title":"4.1 Composio Tools","text":"<p>Composio provides a set of tools that can be used within your GenSphere workflows.</p>"},{"location":"user_guide/integration_with_composio_and_langchain/#using-composio-tools","title":"Using Composio Tools","text":"<ul> <li>Refer to Composio tools in the <code>tools</code> field with the syntax <code>COMPOSIO.tool_name</code>.</li> <li>Ensure you have set your <code>COMPOSIO_API_KEY</code> in your environment variables.</li> </ul> <p>Example YAML Node:</p> <pre><code>- name: web_scraping\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools:\n    - COMPOSIO.FIRECRAWL_SCRAPE\n  params:\n    prompt: |\n      Scrape the content from the following URL:\n      {{ url_to_scrape }}\n  outputs:\n    - scraped_content\n</code></pre>"},{"location":"user_guide/integration_with_composio_and_langchain/#42-langchain-tools","title":"4.2 LangChain Tools","text":"<p>LangChain offers a suite of tools for building applications with language models.</p>"},{"location":"user_guide/integration_with_composio_and_langchain/#using-langchain-tools","title":"Using LangChain Tools","text":"<ul> <li>Refer to LangChain tools in the <code>tools</code> field with the syntax <code>LANGCHAIN.tool_name</code>.</li> <li>Ensure the required LangChain packages are installed.</li> </ul> <p>Example YAML Node:</p> <pre><code>- name: data_analysis\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools:\n    - LANGCHAIN.PandasDataFrameAnalyzer\n  params:\n    prompt: |\n      Analyze the following data:\n      {{ data_frame }}\n  outputs:\n    - analysis_result\n</code></pre>"},{"location":"user_guide/integration_with_composio_and_langchain/#43-custom-tools","title":"4.3 Custom Tools","text":"<p>You can also use custom functions as tools in llm_service nodes.</p> <p>Example:</p> <pre><code>- name: custom_tool_usage\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools:\n    - my_custom_function\n  params:\n    prompt: |\n      Use the custom function to process the data.\n  outputs:\n    - custom_tool_output\n</code></pre> <p>Ensure <code>my_custom_function</code> is defined in your functions file.</p>"},{"location":"user_guide/lists_and_iterables/","title":"Lists and iterables","text":""},{"location":"user_guide/lists_and_iterables/#5-lists-and-iterables","title":"5. Lists and Iterables","text":"<p>Working with lists and iterables in GenSphere allows you to process collections of data efficiently.</p>"},{"location":"user_guide/lists_and_iterables/#51-processing-lists","title":"5.1 Processing Lists","text":"<p>When a node produces a list output, you can process each element individually in subsequent nodes.</p>"},{"location":"user_guide/lists_and_iterables/#referencing-list-elements","title":"Referencing List Elements","text":"<p>Use the <code>[i]</code> syntax to reference individual elements in a list.</p> <p>Example:</p> <pre><code>- name: process_items\n  type: function_call\n  function: process_item_function\n  params:\n    item: '{{ item_list.items[i] }}'\n  outputs:\n    - processed_items\n</code></pre>"},{"location":"user_guide/lists_and_iterables/#52-iterating-over-lists","title":"5.2 Iterating Over Lists","text":"<p>GenSphere automatically detects when you reference a list element and iterates over the list, executing the node for each element.</p>"},{"location":"user_guide/lists_and_iterables/#example-workflow","title":"Example Workflow","text":"<p>Suppose you have a list of items you want to process individually.</p> <p>Node Producing a List:</p> <pre><code>- name: get_items\n  type: function_call\n  function: get_items_function\n  outputs:\n    - items  # This is a list\n</code></pre> <p>Node Processing Each Item:</p> <pre><code>- name: process_each_item\n  type: function_call\n  function: process_item_function\n  params:\n    item: '{{ get_items.items[i] }}'\n  outputs:\n    - processed_items\n</code></pre> <p>Key Points:</p> <ul> <li>The node <code>process_each_item</code> will be executed for each element in <code>get_items.items</code>.</li> <li>The outputs will be collected into a list <code>processed_items</code>.</li> </ul>"},{"location":"user_guide/lists_and_iterables/#53-collecting-outputs","title":"5.3 Collecting Outputs","text":"<p>After iterating over a list, the outputs are collected into a list corresponding to each input element.</p> <p>Accessing Collected Outputs:</p> <pre><code>processed_items = flow.outputs['process_each_item']['processed_items']\nfor item in processed_items:\n    print(item)\n</code></pre>"},{"location":"user_guide/nesting_workflows/","title":"Nesting workflows","text":""},{"location":"user_guide/nesting_workflows/#3-nesting-workflows","title":"3. Nesting Workflows","text":"<p>GenSphere allows you to create modular and reusable workflows by nesting workflows within each other. This is achieved using the <code>yml_flow</code> node type. Nesting workflows can help you organize complex tasks and promote code reuse.</p>"},{"location":"user_guide/nesting_workflows/#31-understanding-yml-flow-nodes","title":"3.1 Understanding YML Flow Nodes","text":"<p>A <code>yml_flow</code> node refers to another YAML workflow file. When the main workflow is executed, GenSphere will incorporate the nested workflow, resolving dependencies and combining them into a single execution graph.</p> <p>Example:</p> <pre><code># main_workflow.yaml\n\nnodes:\n  - name: data_collection\n    type: yml_flow\n    yml_file: data_collection_workflow.yaml\n    params:\n      start_date: '2023-01-01'\n      end_date: '2023-01-31'\n    outputs:\n      - collected_data\n\n  - name: data_analysis\n    type: function_call\n    function: analyze_data_function\n    params:\n      data: '{{ data_collection.collected_data }}'\n    outputs:\n      - analysis_results\n</code></pre> <p>In this example:</p> <ul> <li><code>data_collection</code>: A <code>yml_flow</code> node that references <code>data_collection_workflow.yaml</code>.</li> <li>Parameters: <code>start_date</code> and <code>end_date</code> are passed to the nested workflow.</li> <li>Outputs: <code>collected_data</code> is an output from the nested workflow used in the main workflow.</li> </ul>"},{"location":"user_guide/nesting_workflows/#32-creating-nested-workflows","title":"3.2 Creating Nested Workflows","text":"<p>To create a nested workflow:</p> <ol> <li>Define the Sub-Workflow: Create a separate YAML file (e.g., <code>data_collection_workflow.yaml</code>) with its own nodes.</li> </ol> <p>```yaml    # data_collection_workflow.yaml</p> <p>nodes:      - name: fetch_data        type: function_call        function: fetch_data_function        params:          start_date: '{{ start_date }}'          end_date: '{{ end_date }}'        outputs:          - raw_data</p> <pre><code> - name: process_data\n   type: function_call\n   function: process_data_function\n   params:\n     data: '{{ fetch_data.raw_data }}'\n   outputs:\n     - processed_data\n</code></pre> <p>```</p> <ol> <li>Reference the Sub-Workflow: In your main workflow, use a <code>yml_flow</code> node to include the sub-workflow.</li> </ol>"},{"location":"user_guide/nesting_workflows/#33-passing-parameters-to-nested-workflows","title":"3.3 Passing Parameters to Nested Workflows","text":"<p>Parameters can be passed to nested workflows using the <code>params</code> field in the <code>yml_flow</code> node. These parameters can then be used within the nested workflow.</p> <p>Example:</p> <pre><code># main_workflow.yaml\n\nnodes:\n  - name: sub_workflow\n    type: yml_flow\n    yml_file: sub_workflow.yaml\n    params:\n      param1: '{{ main_param }}'\n</code></pre>"},{"location":"user_guide/nesting_workflows/#34-accessing-outputs-from-nested-workflows","title":"3.4 Accessing Outputs from Nested Workflows","text":"<p>Outputs specified in the <code>outputs</code> field of the <code>yml_flow</code> node are made available to the main workflow.</p> <p>Example:</p> <pre><code>- name: data_collection\n  type: yml_flow\n  yml_file: data_collection_workflow.yaml\n  outputs:\n    - collected_data\n</code></pre> <p>In subsequent nodes, you can reference <code>data_collection.collected_data</code>.</p>"},{"location":"user_guide/nesting_workflows/#35-composing-the-combined-workflow","title":"3.5 Composing the Combined Workflow","text":"<p>Use the <code>YamlCompose</code> class to combine the main workflow and nested workflows into a single executable workflow.</p> <p>Example:</p> <pre><code>from gensphere.yaml_utils import YamlCompose\n\ncomposer = YamlCompose(\n    yaml_file='main_workflow.yaml',\n    functions_filepath='functions.py',\n    structured_output_schema_filepath='schemas.py'\n)\ncombined_yaml_data = composer.compose(save_combined_yaml=True, output_file='combined.yaml')\n</code></pre>"},{"location":"user_guide/user_guide_1/","title":"User Guide","text":"<p>Welcome to the GenSphere User Guide! This section provides detailed information on how to use GenSphere to build and execute complex AI workflows. Whether you're a beginner or an experienced developer, this guide will help you understand the core concepts and features of GenSphere.</p>"},{"location":"user_guide/user_guide_1/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Workflows</li> <li>Functions and Schemas</li> <li>Integrations</li> <li>Lists and Iterables</li> </ol>"},{"location":"user_guide/user_guide_1/#1-workflows","title":"1. Workflows","text":"<p>GenSphere uses YAML files to define workflows that orchestrate the execution of functions, language model services, and nested workflows. Understanding how to structure and write these YAML files is essential to leveraging the full power of GenSphere.</p>"},{"location":"user_guide/user_guide_1/#11-structure-of-a-workflow","title":"1.1 Structure of a Workflow","text":"<p>A GenSphere workflow YAML file consists of a list of nodes, each representing a step in the workflow. Here's the basic structure:</p> <pre><code># example_workflow.yaml\n\nnodes:\n  - name: node_name\n    type: node_type # can be either 'function_call','llm_service' of 'yml_flow'\n    params:\n      param1: value1\n      param2: value2\n    outputs:\n      - output1\n      - output2\n</code></pre> <p>There are additional fields needed depending on the 'type' field.</p>"},{"location":"user_guide/user_guide_1/#12-node-types","title":"1.2 Node Types","text":"<p>There are three primary node types in GenSphere:</p> <ol> <li>Function Call Nodes (<code>function_call</code>): Execute Python functions.</li> <li>LLM Service Nodes (<code>llm_service</code>): Interact with language model APIs.</li> <li>YML Flow Nodes (<code>yml_flow</code>): Nest other YAML workflows.</li> </ol>"},{"location":"user_guide/user_guide_1/#function-call-nodes","title":"Function Call Nodes","text":"<p>Used to execute custom Python functions defined in a separate <code>.py</code> file.</p> <p>Example:</p> <pre><code>- name: process_data\n  type: function_call\n  function: process_data_function\n  params:\n    data: '{{ previous_node.output }}'\n  outputs:\n    - processed_data\n</code></pre> <p>The function <code>process_data_function</code> should be defined on a separate <code>.py</code> file that contains all functions that you will use. We don't set the path of this <code>.py</code> file directly in the YAML, but pass it as na argument to GenFlow when executing the graph (as explained below).</p>"},{"location":"user_guide/user_guide_1/#llm-service-nodes","title":"LLM Service Nodes","text":"<p>Used to interact with language model services like OpenAI's GPT models.</p> <p>Example:</p> <pre><code>- name: generate_summary\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools: \n    - COMPOSIO.composio_tool_name\n    - LANGCHAIN.langchain_tool_name\n    - custom_function\n  params:\n    prompt: |\n      Summarize the following data:\n      {{ process_data.processed_data }}\n  outputs:\n    - summary\n</code></pre> <p>In the example above, we are making na API call to the <code>chat_completions</code> API from openAI, passing the prompt described. Notice that we can reference others nodes outputs directly in the prompt (they are turned to strings when executing this node). </p> <p>Also, notice that we are passing several tools for function calling (which are optional).  You can use Composio tools with the syntax COMPOSIO.composio_tool_name.  You can use any tool from langchain_community.tools with the syntax \"LANGCHAIN.langchain_tool_name\".</p> <p>Besides function calling, we can also work with structured outputs. Structured outputs is a great  practical feature that allows you to enforce a predetermined schema from the openAI API call. Using it ensures the model will always generate responses that adhere to your supplied schema. So, if you want the node output to be of the type</p> <p><code>{'field_1':value_1, 'field_2':value_2}</code></p> <p>where the values are strings, you first define a pydantic model that defines the schema on a separate schemas.py file</p> <p>`` from pydantic import BaseModel</p> <p>class ResponseSchema(BaseModel):,             field_1: str = Field(..., description=\"Field 1 description\"),             field_2: str = Field(..., description=\"Field 2 description\") ``</p> <p>and then pass it in the YAML file as a field in the llm_service node</p> <pre><code>- name: generate_summary\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  structured_output_schema: ResponseSchema\n  params:\n    prompt: |\n      Summarize the following data:\n      {{ process_data.processed_data }}\n  outputs:\n    - summary\n</code></pre> <p>The output of this node will then be an instance of the class ResponseSchema that we defined. If we want to transform this to a dictionary or do any other further post-processing, we should create a separate node for that. </p> <p>Like when workinf with functions, we don't pass the path of the schemas.py file directly in the YAML file, but will do it when using <code>GenFlow</code> to execute the workflow. </p>"},{"location":"user_guide/user_guide_1/#yml-flow-nodes","title":"YML Flow Nodes","text":"<p>Used to include and execute another YAML workflow within the current workflow.</p> <p>Example:</p> <pre><code>- name: sub_workflow\n  type: yml_flow\n  yml_file: sub_workflow.yaml\n  params:\n    input_data: '{{ generate_summary.summary }}'\n  outputs:\n    - subflow_output\n</code></pre> <p>This will trigger the entire execution of the workflow defined on <code>sub_workflow.yaml</code>. The path on the <code>yml_file</code> field should be relative to where the parente YAML file is located.</p>"},{"location":"user_guide/user_guide_1/#13-parameters-and-outputs","title":"1.3 Parameters and Outputs","text":"<ul> <li>Parameters (<code>params</code>): Inputs required for the node to execute. You can reference outputs from previous nodes using the syntax <code>{{ node_name.output_name }}</code>.</li> <li>Outputs (<code>outputs</code>): The data produced by the node, which can be used by subsequent nodes.</li> </ul>"},{"location":"user_guide/user_guide_1/#14-referencing-outputs","title":"1.4 Referencing Outputs","text":"<p>Use the Jinja2 templating syntax to reference outputs from other nodes:</p> <pre><code>params:\n  input_data: '{{ previous_node.output_name }}'\n</code></pre>"},{"location":"user_guide/user_guide_1/#2-functions-and-schemas","title":"2. Functions and Schemas","text":"<p>In GenSphere, you can define custom functions and schemas to extend the functionality of your workflows.</p>"},{"location":"user_guide/user_guide_1/#21-functions","title":"2.1 Functions","text":"<p>Custom functions are Python functions defined in a <code>.py</code> file that you specify when running your workflow. These functions can be used in function_call nodes or as tools in llm_service nodes.</p>"},{"location":"user_guide/user_guide_1/#defining-functions","title":"Defining Functions","text":"<ul> <li>Functions must return a dictionary whose keys match the <code>outputs</code> defined in the YAML node.</li> <li>All parameters must be type-annotated.</li> <li>Include docstrings to describe the function's purpose, parameters, and return values.</li> </ul> <p>Example (<code>functions.py</code>):</p> <pre><code>def process_data_function(data: str) -&gt; dict:\n    \"\"\"\n    Processes the input data.\n\n    Args:\n        data (str): The data to process.\n\n    Returns:\n        dict: A dictionary with 'processed_data' as key.\n    \"\"\"\n    # Your processing logic here\n    processed_data = data.upper()  # Example processing\n    return {'processed_data': processed_data}\n</code></pre>"},{"location":"user_guide/user_guide_1/#using-functions-in-nodes","title":"Using Functions in Nodes","text":"<p>Example YAML Node:</p> <pre><code>- name: process_data\n  type: function_call\n  function: process_data_function\n  params:\n    data: '{{ read_data.raw_data }}'\n  outputs:\n    - processed_data\n</code></pre>"},{"location":"user_guide/user_guide_1/#22-schemas","title":"2.2 Schemas","text":"<p>Schemas are used to define structured outputs for language model responses using Pydantic models.</p>"},{"location":"user_guide/user_guide_1/#defining-schemas","title":"Defining Schemas","text":"<p>Create a <code>.py</code> file (e.g., <code>schemas.py</code>) with Pydantic models representing the expected structure of your data.</p> <p>Example (<code>schemas.py</code>):</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass Item(BaseModel):\n    id: int = Field(..., description=\"The unique identifier of the item.\")\n    name: str = Field(..., description=\"The name of the item.\")\n    description: str = Field(..., description=\"A brief description of the item.\")\n\nclass ItemList(BaseModel):\n    items: List[Item]\n</code></pre>"},{"location":"user_guide/user_guide_1/#using-schemas-in-nodes","title":"Using Schemas in Nodes","text":"<p>Use the <code>structured_output_schema</code> field in an llm_service node to specify the schema.</p> <p>Example YAML Node:</p> <pre><code>- name: extract_items\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  structured_output_schema: ItemList\n  params:\n    prompt: |\n      Extract item information from the following text:\n      {{ source_text }}\n  outputs:\n    - extracted_items\n</code></pre> <p>Key Points:</p> <ul> <li>The <code>structured_output_schema</code> value should match the class name in your schema file.</li> <li>The language model will attempt to produce output that conforms to the specified schema.</li> </ul>"},{"location":"user_guide/user_guide_1/#accessing-structured-outputs","title":"Accessing Structured Outputs","text":"<p>After execution, the output will be an instance of the schema class. You can access the data using standard attribute access.</p> <p>Example:</p> <pre><code>items = flow.outputs['extract_items']['extracted_items']\nfor item in items.items:\n    print(f\"Item ID: {item.id}, Name: {item.name}\")\n</code></pre>"},{"location":"user_guide/user_guide_1/#3-integrations","title":"3. Integrations","text":"<p>GenSphere supports integration with external tools and platforms to enhance your workflows.</p>"},{"location":"user_guide/user_guide_1/#31-composio-tools","title":"3.1 Composio Tools","text":"<p>Composio provides a set of tools that can be used within your GenSphere workflows.</p>"},{"location":"user_guide/user_guide_1/#using-composio-tools","title":"Using Composio Tools","text":"<ul> <li>Refer to Composio tools in the <code>tools</code> field with the syntax <code>COMPOSIO.tool_name</code>.</li> <li>Ensure you have set your <code>COMPOSIO_API_KEY</code> in your environment variables.</li> </ul> <p>Example YAML Node:</p> <pre><code>- name: web_scraping\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools:\n    - COMPOSIO.FIRECRAWL_SCRAPE\n  params:\n    prompt: |\n      Scrape the content from the following URL:\n      {{ url_to_scrape }}\n  outputs:\n    - scraped_content\n</code></pre>"},{"location":"user_guide/user_guide_1/#32-langchain-tools","title":"3.2 LangChain Tools","text":"<p>LangChain offers a suite of tools for building applications with language models.</p>"},{"location":"user_guide/user_guide_1/#using-langchain-tools","title":"Using LangChain Tools","text":"<ul> <li>Refer to LangChain tools in the <code>tools</code> field with the syntax <code>LANGCHAIN.tool_name</code>.</li> <li>Ensure the required LangChain packages are installed.</li> </ul> <p>Example YAML Node:</p> <pre><code>- name: data_analysis\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools:\n    - LANGCHAIN.PandasDataFrameAnalyzer\n  params:\n    prompt: |\n      Analyze the following data:\n      {{ data_frame }}\n  outputs:\n    - analysis_result\n</code></pre>"},{"location":"user_guide/user_guide_1/#33-custom-tools","title":"3.3 Custom Tools","text":"<p>You can also use custom functions as tools in llm_service nodes.</p> <p>Example:</p> <pre><code>- name: custom_tool_usage\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools:\n    - my_custom_function\n  params:\n    prompt: |\n      Use the custom function to process the data.\n  outputs:\n    - custom_tool_output\n</code></pre> <p>Ensure <code>my_custom_function</code> is defined in your functions file.</p>"},{"location":"user_guide/user_guide_1/#4-lists-and-iterables","title":"4. Lists and Iterables","text":"<p>Working with lists and iterables in GenSphere allows you to process collections of data efficiently.</p>"},{"location":"user_guide/user_guide_1/#41-processing-lists","title":"4.1 Processing Lists","text":"<p>When a node produces a list output, you can process each element individually in subsequent nodes.</p>"},{"location":"user_guide/user_guide_1/#referencing-list-elements","title":"Referencing List Elements","text":"<p>Use the <code>[i]</code> syntax to reference individual elements in a list.</p> <p>Example:</p> <pre><code>- name: process_items\n  type: function_call\n  function: process_item_function\n  params:\n    item: '{{ item_list.items[i] }}'\n  outputs:\n    - processed_items\n</code></pre>"},{"location":"user_guide/user_guide_1/#42-iterating-over-lists","title":"4.2 Iterating Over Lists","text":"<p>GenSphere automatically detects when you reference a list element and iterates over the list, executing the node for each element.</p>"},{"location":"user_guide/user_guide_1/#example-workflow","title":"Example Workflow","text":"<p>Suppose you have a list of items you want to process individually.</p> <p>Node Producing a List:</p> <pre><code>- name: get_items\n  type: function_call\n  function: get_items_function\n  outputs:\n    - items  # This is a list\n</code></pre> <p>Node Processing Each Item:</p> <pre><code>- name: process_each_item\n  type: function_call\n  function: process_item_function\n  params:\n    item: '{{ get_items.items[i] }}'\n  outputs:\n    - processed_items\n</code></pre> <p>Key Points:</p> <ul> <li>The node <code>process_each_item</code> will be executed for each element in <code>get_items.items</code>.</li> <li>The outputs will be collected into a list <code>processed_items</code>.</li> </ul>"},{"location":"user_guide/user_guide_1/#43-collecting-outputs","title":"4.3 Collecting Outputs","text":"<p>After iterating over a list, the outputs are collected into a list corresponding to each input element.</p> <p>Accessing Collected Outputs:</p> <pre><code>processed_items = flow.outputs['process_each_item']['processed_items']\nfor item in processed_items:\n    print(item)\n</code></pre>"},{"location":"user_guide/user_guide_1/#44-nested-iterations","title":"4.4 Nested Iterations","text":"<p>You can perform nested iterations by referencing elements within elements.</p> <p>Example:</p> <pre><code>- name: process_nested_items\n  type: function_call\n  function: process_nested_function\n  params:\n    sub_item: '{{ get_items.items[i].sub_items[j] }}'\n  outputs:\n    - nested_processed_items\n</code></pre> <p>Note: Be cautious with nested iterations as they can significantly increase the number of executions.</p>"},{"location":"user_guide/user_guide_1/#next-steps","title":"Next Steps","text":"<p>Now that you've learned about workflows, functions and schemas, integrations, and working with lists, you're ready to explore more advanced features of GenSphere:</p> <ul> <li>Nesting Workflows: Learn how to create modular and reusable workflows.</li> <li>Visualization: Understand how to visualize workflows for better insight.</li> <li>Execution: Dive deeper into executing workflows and handling outputs.</li> <li>Using the Hub: Discover how to share and access workflows on the GenSphere platform.</li> </ul>"},{"location":"user_guide/user_guide_1/#additional-resources","title":"Additional Resources","text":"<ul> <li>GenSphere GitHub Repository</li> <li>GenSphere Documentation</li> <li>Composio SDK Guide</li> <li>LangChain Tools API Reference</li> </ul>"},{"location":"user_guide/user_guide_1/#feedback","title":"Feedback","text":"<p>We hope this guide helps you get started with GenSphere. If you have any questions or feedback, please reach out on our GitHub Issues page.</p>"},{"location":"user_guide/user_guide_1/#end-of-user-guide-sections","title":"End of User Guide Sections","text":"<p>Feel free to explore the next sections in the user guide for more detailed information on nesting workflows, visualization, execution, and using the GenSphere Hub.</p>"},{"location":"user_guide/user_guide_2/","title":"User Guide","text":"<p>Welcome back to the GenSphere User Guide! In this section, we'll continue exploring the advanced features of GenSphere, focusing on nesting workflows, visualization, execution, and using the GenSphere Hub.</p>"},{"location":"user_guide/user_guide_2/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Nesting Workflows</li> <li>Visualization</li> <li>Execution</li> <li>Using the GenSphere Hub</li> </ol>"},{"location":"user_guide/user_guide_2/#1-nesting-workflows","title":"1. Nesting Workflows","text":"<p>GenSphere allows you to create modular and reusable workflows by nesting workflows within each other. This is achieved using the <code>yml_flow</code> node type. Nesting workflows can help you organize complex tasks and promote code reuse.</p>"},{"location":"user_guide/user_guide_2/#11-understanding-yml-flow-nodes","title":"1.1 Understanding YML Flow Nodes","text":"<p>A <code>yml_flow</code> node refers to another YAML workflow file. When the main workflow is executed, GenSphere will incorporate the nested workflow, resolving dependencies and combining them into a single execution graph.</p> <p>Example:</p> <pre><code># main_workflow.yaml\n\nnodes:\n  - name: data_collection\n    type: yml_flow\n    yml_file: data_collection_workflow.yaml\n    params:\n      start_date: '2023-01-01'\n      end_date: '2023-01-31'\n    outputs:\n      - collected_data\n\n  - name: data_analysis\n    type: function_call\n    function: analyze_data_function\n    params:\n      data: '{{ data_collection.collected_data }}'\n    outputs:\n      - analysis_results\n</code></pre> <p>In this example:</p> <ul> <li><code>data_collection</code>: A <code>yml_flow</code> node that references <code>data_collection_workflow.yaml</code>.</li> <li>Parameters: <code>start_date</code> and <code>end_date</code> are passed to the nested workflow.</li> <li>Outputs: <code>collected_data</code> is an output from the nested workflow used in the main workflow.</li> </ul>"},{"location":"user_guide/user_guide_2/#12-creating-nested-workflows","title":"1.2 Creating Nested Workflows","text":"<p>To create a nested workflow:</p> <ol> <li>Define the Sub-Workflow: Create a separate YAML file (e.g., <code>data_collection_workflow.yaml</code>) with its own nodes.</li> </ol> <p>```yaml    # data_collection_workflow.yaml</p> <p>nodes:      - name: fetch_data        type: function_call        function: fetch_data_function        params:          start_date: '{{ start_date }}'          end_date: '{{ end_date }}'        outputs:          - raw_data</p> <pre><code> - name: process_data\n   type: function_call\n   function: process_data_function\n   params:\n     data: '{{ fetch_data.raw_data }}'\n   outputs:\n     - processed_data\n</code></pre> <p>```</p> <ol> <li>Reference the Sub-Workflow: In your main workflow, use a <code>yml_flow</code> node to include the sub-workflow.</li> </ol>"},{"location":"user_guide/user_guide_2/#13-passing-parameters-to-nested-workflows","title":"1.3 Passing Parameters to Nested Workflows","text":"<p>Parameters can be passed to nested workflows using the <code>params</code> field in the <code>yml_flow</code> node. These parameters can then be used within the nested workflow.</p> <p>Example:</p> <pre><code># main_workflow.yaml\n\nnodes:\n  - name: sub_workflow\n    type: yml_flow\n    yml_file: sub_workflow.yaml\n    params:\n      param1: '{{ main_param }}'\n</code></pre>"},{"location":"user_guide/user_guide_2/#14-accessing-outputs-from-nested-workflows","title":"1.4 Accessing Outputs from Nested Workflows","text":"<p>Outputs specified in the <code>outputs</code> field of the <code>yml_flow</code> node are made available to the main workflow.</p> <p>Example:</p> <pre><code>- name: data_collection\n  type: yml_flow\n  yml_file: data_collection_workflow.yaml\n  outputs:\n    - collected_data\n</code></pre> <p>In subsequent nodes, you can reference <code>data_collection.collected_data</code>.</p>"},{"location":"user_guide/user_guide_2/#15-composing-the-combined-workflow","title":"1.5 Composing the Combined Workflow","text":"<p>Use the <code>YamlCompose</code> class to combine the main workflow and nested workflows into a single executable workflow.</p> <p>Example:</p> <pre><code>from gensphere.yaml_utils import YamlCompose\n\ncomposer = YamlCompose(\n    yaml_file='main_workflow.yaml',\n    functions_filepath='functions.py',\n    structured_output_schema_filepath='schemas.py'\n)\ncombined_yaml_data = composer.compose(save_combined_yaml=True, output_file='combined.yaml')\n</code></pre>"},{"location":"user_guide/user_guide_2/#2-visualization","title":"2. Visualization","text":"<p>Visualizing your workflow helps you understand the execution flow and dependencies between nodes. GenSphere provides a visualization tool to render your workflow as an interactive graph.</p>"},{"location":"user_guide/user_guide_2/#21-using-the-visualizer","title":"2.1 Using the Visualizer","text":"<p>The <code>Visualizer</code> class in GenSphere allows you to visualize your workflow.</p> <p>Example:</p> <pre><code>from gensphere.visualizer import Visualizer\n\nviz = Visualizer(\n    yaml_file='combined.yaml',\n    functions_file='functions.py',\n    schema_file='schemas.py',\n    address='127.0.0.1',\n    port=8050\n)\nviz.start_visualization()\n</code></pre>"},{"location":"user_guide/user_guide_2/#22-features-of-the-visualizer","title":"2.2 Features of the Visualizer","text":"<ul> <li>Interactive Graph: Nodes are displayed as a graph where you can zoom in/out.</li> <li>Node Details: Click on a node to see details such as inputs, outputs, functions, and schemas.</li> <li>Execution Flow: Visualize the dependencies and execution order.</li> </ul>"},{"location":"user_guide/user_guide_2/#23-running-the-visualizer","title":"2.3 Running the Visualizer","text":"<p>When you run <code>viz.start_visualization()</code>, the visualizer starts a local web server. Open your browser and navigate to <code>http://127.0.0.1:8050</code> (or the specified address and port) to view the visualization.</p> <p>Note: If you're running in an environment like Google Colab, you might need to set up port forwarding or use a different method to access the visualization.</p>"},{"location":"user_guide/user_guide_2/#3-execution","title":"3. Execution","text":"<p>Executing workflows in GenSphere is straightforward using the <code>GenFlow</code> class. This section explains how to execute your workflow and handle outputs.</p>"},{"location":"user_guide/user_guide_2/#31-setting-up-the-environment","title":"3.1 Setting Up the Environment","text":"<p>Ensure that:</p> <ul> <li>All required environment variables (e.g., API keys) are set.</li> <li>All necessary files (<code>yaml</code>, <code>functions.py</code>, <code>schemas.py</code>) are available.</li> </ul>"},{"location":"user_guide/user_guide_2/#32-executing-the-workflow","title":"3.2 Executing the Workflow","text":"<p>Use the <code>GenFlow</code> class to execute your workflow.</p> <p>Example:</p> <pre><code>from gensphere.genflow import GenFlow\n\nflow = GenFlow(\n    yaml_file='combined.yaml',\n    functions_filepath='functions.py',\n    structured_output_schema_filepath='schemas.py'\n)\nflow.parse_yaml()\nflow.run()\n</code></pre>"},{"location":"user_guide/user_guide_2/#33-accessing-outputs","title":"3.3 Accessing Outputs","text":"<p>After execution, outputs from each node are accessible via the <code>outputs</code> attribute.</p> <p>Example:</p> <pre><code># Access all outputs\noutputs = flow.outputs\n\n# Access specific node outputs\ndata_collection_output = outputs.get('data_collection').get('collected_data')\n\n# Print the outputs\nprint(\"Collected Data:\")\nprint(data_collection_output)\n</code></pre>"},{"location":"user_guide/user_guide_2/#34-handling-exceptions","title":"3.4 Handling Exceptions","text":"<p>If an error occurs during execution:</p> <ul> <li>Check the log files for detailed error messages.</li> <li>Ensure that all referenced nodes, functions, and schemas exist.</li> <li>Verify that all parameters are correctly specified.</li> </ul>"},{"location":"user_guide/user_guide_2/#35-logging","title":"3.5 Logging","text":"<p>GenSphere uses Python's logging module to log execution details.</p> <ul> <li>Logging Levels: Adjust logging levels as needed.   <code>python   import logging   logging.getLogger('gensphere').setLevel(logging.DEBUG)</code></li> <li>Log File: By default, logs may be written to <code>app.log</code> or the console.</li> </ul>"},{"location":"user_guide/user_guide_2/#36-execution-flow","title":"3.6 Execution Flow","text":"<p>The execution order of nodes is determined by their dependencies:</p> <ul> <li>Nodes with no dependencies are executed first.</li> <li>Nodes are executed after all their dependencies have completed.</li> </ul>"},{"location":"user_guide/user_guide_2/#4-using-the-gensphere-hub","title":"4. Using the GenSphere Hub","text":"<p>The GenSphere Hub is a platform that allows you to share your workflows with the community and access workflows shared by others.</p>"},{"location":"user_guide/user_guide_2/#41-pushing-workflows-to-the-hub","title":"4.1 Pushing Workflows to the Hub","text":"<p>To share your workflow, use the <code>Hub</code> class to push your files to the platform.</p> <p>Example:</p> <pre><code>from gensphere.hub import Hub\n\nhub = Hub(\n    yaml_file='combined.yaml',\n    functions_file='functions.py',\n    schema_file='schemas.py'\n)\n\nresult = hub.push(push_name='My Awesome Workflow')\npush_id = result.get('push_id')\nprint(f\"Your workflow has been pushed with push_id: {push_id}\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>push_name</code>: A descriptive name for your workflow.</li> <li><code>yaml_file</code>, <code>functions_file</code>, <code>schema_file</code>: Paths to your workflow files.</li> </ul>"},{"location":"user_guide/user_guide_2/#42-pulling-workflows-from-the-hub","title":"4.2 Pulling Workflows from the Hub","text":"<p>To use a workflow shared by someone else, you can pull it using the <code>push_id</code>.</p> <p>Example:</p> <pre><code>hub = Hub()\nhub.pull(\n    push_id='abcd1234-5678-90ef-ghij-klmnopqrstuv',\n    yaml_filename='downloaded_workflow.yaml',\n    functions_filename='downloaded_functions.py',\n    schema_filename='downloaded_schemas.py',\n    save_to_disk=True\n)\n</code></pre>"},{"location":"user_guide/user_guide_2/#43-counting-pulls","title":"4.3 Counting Pulls","text":"<p>You can check how many times your workflow has been pulled.</p> <p>Example:</p> <pre><code>total_pulls = hub.count_pulls(push_id=push_id)\nprint(f\"Your workflow has been pulled {total_pulls} times.\")\n</code></pre>"},{"location":"user_guide/user_guide_2/#44-deleting-a-workflow-from-the-hub","title":"4.4 Deleting a Workflow from the Hub","text":"<p>If you need to remove your workflow from the Hub, you can delete it.</p> <p>Example:</p> <pre><code>hub.delete_push(push_id=push_id)\nprint(\"Your workflow has been deleted from the Hub.\")\n</code></pre> <p>Note: Ensure you have the necessary permissions to delete the workflow.</p>"},{"location":"user_guide/user_guide_2/#45-best-practices-for-sharing-workflows","title":"4.5 Best Practices for Sharing Workflows","text":"<ul> <li>Descriptive Names: Use clear and descriptive names for your workflows.</li> <li>Documentation: Include comments or documentation within your functions and schemas to help others understand your workflow.</li> <li>Dependencies: Ensure all dependencies are included or documented.</li> </ul>"},{"location":"user_guide/user_guide_2/#46-privacy-and-security","title":"4.6 Privacy and Security","text":"<ul> <li>Sensitive Data: Do not include API keys or sensitive information in your shared workflows.</li> <li>Licensing: Respect licensing agreements and include appropriate licenses if necessary.</li> </ul>"},{"location":"user_guide/user_guide_2/#conclusion","title":"Conclusion","text":"<p>You've now learned how to:</p> <ul> <li>Nest Workflows: Create modular and reusable workflows using <code>yml_flow</code> nodes.</li> <li>Visualize Workflows: Use the <code>Visualizer</code> to understand and debug your workflows.</li> <li>Execute Workflows: Run your workflows using the <code>GenFlow</code> class and handle outputs.</li> <li>Use the GenSphere Hub: Share your workflows with the community and access others' workflows.</li> </ul>"},{"location":"user_guide/user_guide_2/#additional-resources","title":"Additional Resources","text":"<ul> <li>GenSphere GitHub Repository</li> <li>GenSphere Documentation</li> <li>Community Forum</li> <li>Issue Tracker</li> </ul>"},{"location":"user_guide/user_guide_2/#feedback","title":"Feedback","text":"<p>We value your feedback! If you have suggestions, questions, or need help, please reach out on our GitHub Issues page or join the community forum.</p>"},{"location":"user_guide/user_guide_2/#end-of-user-guide-sections","title":"End of User Guide Sections","text":"<p>You're now equipped with the knowledge to fully utilize GenSphere's capabilities. Happy coding!</p>"},{"location":"user_guide/visualization/","title":"Visualization","text":""},{"location":"user_guide/visualization/#6-visualization","title":"6. Visualization","text":"<p>Visualizing your workflow helps you understand the execution flow and dependencies between nodes. GenSphere provides a visualization tool to render your workflow as an interactive graph.</p>"},{"location":"user_guide/visualization/#61-using-the-visualizer","title":"6.1 Using the Visualizer","text":"<p>The <code>Visualizer</code> class in GenSphere allows you to visualize your workflow.</p> <p>Example:</p> <pre><code>from gensphere.visualizer import Visualizer\n\nviz = Visualizer(\n    yaml_file='combined.yaml',\n    functions_file='functions.py',\n    schema_file='schemas.py',\n    address='127.0.0.1',\n    port=8050\n)\nviz.start_visualization()\n</code></pre>"},{"location":"user_guide/visualization/#62-features-of-the-visualizer","title":"6.2 Features of the Visualizer","text":"<ul> <li>Interactive Graph: Nodes are displayed as a graph where you can zoom in/out.</li> <li>Node Details: Click on a node to see details such as inputs, outputs, functions, and schemas.</li> <li>Execution Flow: Visualize the dependencies and execution order.</li> </ul>"},{"location":"user_guide/visualization/#63-running-the-visualizer","title":"6.3 Running the Visualizer","text":"<p>When you run <code>viz.start_visualization()</code>, the visualizer starts a local web server. Open your browser and navigate to <code>http://127.0.0.1:8050</code> (or the specified address and port) to view the visualization.</p> <p>Note: If you're running in an environment like Google Colab, you might need to set up port forwarding or use a different method to access the visualization.</p>"},{"location":"user_guide/workflows/","title":"Workflows","text":""},{"location":"user_guide/workflows/#1-workflows","title":"1. Workflows","text":"<p>GenSphere uses YAML files to define workflows that orchestrate the execution of functions, language model services, and nested workflows. Understanding how to structure and write these YAML files is essential to leveraging the full power of GenSphere.</p>"},{"location":"user_guide/workflows/#11-structure-of-a-workflow","title":"1.1 Structure of a Workflow","text":"<p>A GenSphere workflow YAML file consists of a list of nodes, each representing a step in the workflow. Here's the basic structure:</p> <pre><code># example_workflow.yaml\n\nnodes:\n  - name: node_name\n    type: node_type # can be either 'function_call','llm_service' of 'yml_flow'\n    params:\n      param1: value1\n      param2: value2\n    outputs:\n      - output1\n      - output2\n</code></pre> <p>There are additional fields needed depending on the 'type' field.</p>"},{"location":"user_guide/workflows/#12-node-types","title":"1.2 Node Types","text":"<p>There are three primary node types in GenSphere:</p> <ol> <li>Function Call Nodes (<code>function_call</code>): Execute Python functions.</li> <li>LLM Service Nodes (<code>llm_service</code>): Interact with language model APIs.</li> <li>YML Flow Nodes (<code>yml_flow</code>): Nest other YAML workflows.</li> </ol>"},{"location":"user_guide/workflows/#function-call-nodes","title":"Function Call Nodes","text":"<p>Used to execute custom Python functions defined in a separate <code>.py</code> file.</p> <p>Example:</p> <pre><code>- name: process_data\n  type: function_call\n  function: process_data_function\n  params:\n    data: '{{ previous_node.output }}'\n  outputs:\n    - processed_data\n</code></pre> <p>The function <code>process_data_function</code> should be defined on a separate <code>.py</code> file that contains all functions that you will use. We don't set the path of this <code>.py</code> file directly in the YAML, but pass it as na argument to GenFlow when executing the graph (as explained below).</p>"},{"location":"user_guide/workflows/#llm-service-nodes","title":"LLM Service Nodes","text":"<p>Used to interact with language model services like OpenAI's GPT models.</p> <p>Example:</p> <pre><code>- name: generate_summary\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  tools: \n    - COMPOSIO.composio_tool_name\n    - LANGCHAIN.langchain_tool_name\n    - custom_function\n  params:\n    prompt: |\n      Summarize the following data:\n      {{ process_data.processed_data }}\n  outputs:\n    - summary\n</code></pre> <p>In the example above, we are making na API call to the <code>chat_completions</code> API from openAI, passing the prompt described. Notice that we can reference others nodes outputs directly in the prompt (they are turned to strings when executing this node). </p> <p>Also, notice that we are passing several tools for function calling (which are optional).  You can use Composio tools with the syntax COMPOSIO.composio_tool_name.  You can use any tool from langchain_community.tools with the syntax \"LANGCHAIN.langchain_tool_name\".</p> <p>Besides function calling, we can also work with structured outputs. Structured outputs is a great  practical feature that allows you to enforce a predetermined schema from the openAI API call. Using it ensures the model will always generate responses that adhere to your supplied schema. So, if you want the node output to be of the type</p> <p><code>{'field_1':value_1, 'field_2':value_2}</code></p> <p>where the values are strings, you first define a pydantic model that defines the schema on a separate schemas.py file</p> <p>`` from pydantic import BaseModel</p> <p>class ResponseSchema(BaseModel):,             field_1: str = Field(..., description=\"Field 1 description\"),             field_2: str = Field(..., description=\"Field 2 description\") ``</p> <p>and then pass it in the YAML file as a field in the llm_service node</p> <pre><code>- name: generate_summary\n  type: llm_service\n  service: openai\n  model: \"gpt-4\"\n  structured_output_schema: ResponseSchema\n  params:\n    prompt: |\n      Summarize the following data:\n      {{ process_data.processed_data }}\n  outputs:\n    - summary\n</code></pre> <p>The output of this node will then be an instance of the class ResponseSchema that we defined. If we want to transform this to a dictionary or do any other further post-processing, we should create a separate node for that. </p> <p>Like when workinf with functions, we don't pass the path of the schemas.py file directly in the YAML file, but will do it when using <code>GenFlow</code> to execute the workflow. </p>"},{"location":"user_guide/workflows/#yml-flow-nodes","title":"YML Flow Nodes","text":"<p>Used to include and execute another YAML workflow within the current workflow.</p> <p>Example:</p> <pre><code>- name: sub_workflow\n  type: yml_flow\n  yml_file: sub_workflow.yaml\n  params:\n    input_data: '{{ generate_summary.summary }}'\n  outputs:\n    - subflow_output\n</code></pre> <p>This will trigger the entire execution of the workflow defined on <code>sub_workflow.yaml</code>. The path on the <code>yml_file</code> field should be relative to where the parente YAML file is located.</p>"},{"location":"user_guide/workflows/#13-parameters-and-outputs","title":"1.3 Parameters and Outputs","text":"<ul> <li>Parameters (<code>params</code>): Inputs required for the node to execute. You can reference outputs from previous nodes using the syntax <code>{{ node_name.output_name }}</code>.</li> <li>Outputs (<code>outputs</code>): The data produced by the node, which can be used by subsequent nodes.</li> </ul>"},{"location":"user_guide/workflows/#14-referencing-outputs","title":"1.4 Referencing Outputs","text":"<p>Use the Jinja2 templating syntax to reference outputs from other nodes:</p> <pre><code>params:\n  input_data: '{{ previous_node.output_name }}'\n</code></pre>"}]}